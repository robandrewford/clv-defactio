# Pipeline Configuration Settings

# Data Processing Pipeline
data_processing:
  # Input data settings
  input:
    source_type: bigquery
    project_id: "logic-dna-240402"
    dataset_id: clv_dataset
    table_id: transactions
    batch_size: 1000
    max_workers: 4

  # Data cleaning settings
  cleaning:
    remove_duplicates: true
    handle_missing_values: true
    remove_outliers: true
    outlier_method: iqr # or 'zscore', 'isolation_forest'
    outlier_threshold: 3.0

  # Data validation settings
  validation:
    validate_schema: true
    validate_datatypes: true
    min_rows: 10
    max_missing_pct: 1.0
    required_columns:
      - customer_id
      - transaction_date
      - transaction_amount

# Feature Engineering Pipeline
feature_engineering:
  # Time-based features
  time_features:
    enable: true
    features:
      - day_of_week
      - month
      - quarter
      - is_weekend
      - is_holiday

  # Customer features
  customer_features:
    enable: true
    features:
      - purchase_frequency
      - average_order_value
      - customer_lifetime
      - days_since_last_purchase

  # Product features
  product_features:
    enable: true
    features:
      - category_diversity
      - brand_loyalty
      - price_sensitivity

# Vertex AI Pipeline Settings
vertex_pipeline:
  project_id: "logic-dna-240402"
  region: "us-west1"
  pipeline_root: gs://your-bucket/pipeline_root

  # Pipeline resources
  resources:
    machine_type: "n1-standard-4"
    accelerator_type: "NVIDIA_TESLA_T4"
    accelerator_count: 1

  # Pipeline scheduling
  scheduling:
    enable_cron: true
    cron_schedule: "0 0 * * *" # Daily at midnight
    timeout_seconds: 3600

  # Pipeline monitoring
  monitoring:
    enable_monitoring: true
    alert_email: alerts@your-domain.com
    metrics:
      - pipeline_status
      - processing_time
      - error_rate
      - data_drift
    metrics_frequency: 60
    alert_thresholds:
      error_rate: 0.01
      latency: 300

# Storage Configuration
storage:
  # GCS settings
  gcs:
    bucket_name: your-clv-bucket
    data_prefix: processed_data
    model_prefix: trained_models

  # BigQuery settings
  bigquery:
    dataset_id: clv_results
    results_table: model_predictions
    metrics_table: model_metrics

  model_storage:
    type: "local"
    path: "models"
    bucket_name: "default-bucket"

# Logging Configuration
logging:
  level: INFO
  log_to_file: true
  log_path: /var/log/clv_pipeline
  enable_cloud_logging: true

# Error Handling
error_handling:
  retry_count: 3
  retry_delay_seconds: 60
  alert_on_failure: true
  fallback_strategy: skip # or 'retry', 'abort'

# Performance Settings
performance:
  enable_caching: true
  cache_ttl_hours: 24
  parallel_processing: true
  max_parallel_jobs: 4
  memory_limit_gb: 4

# Security Settings
security:
  enable_encryption: true
  encryption_key_name: projects/your-project/locations/global/keyRings/clv-keyring/cryptoKeys/clv-key
  use_cmek: true # Customer-managed encryption keys
  required_roles:
    - roles/bigquery.dataViewer
    - roles/storage.objectViewer
    - roles/aiplatform.user

# Visualization Settings
visualization:
  style: "default"
  dpi: 100
  formats:
    - "png"
    - "pdf"

  trace_plots:
    figsize: [12, 8]
    n_chains_display: 4
    hist_bins: 30

  segment_plots:
    figsize: [15, 6]
    palette: "deep"
    bar_alpha: 0.8

  prediction_plots:
    figsize: [10, 6]
    ci_alpha: 0.3
    line_color: "blue"
    ci_color: "lightblue"

  diagnostic_plots:
    figsize: [12, 8]
    rhat_threshold: 1.1
    hist_bins: 20
    grid: true
