{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "TsHZhAVbZcVi",
   "metadata": {
    "id": "TsHZhAVbZcVi"
   },
   "outputs": [],
   "source": [
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Rrj6NI-OBnDz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rrj6NI-OBnDz",
    "outputId": "8b84aa54-83b6-41e1-ea64-d0f62bc06956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Imports\n",
    "## 1.2.1 Import Libraries\n",
    "import pymc as pm\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pkg_resources\n",
    "import jax\n",
    "import numpyro\n",
    "import torch\n",
    "import pytensor.tensor as pt\n",
    "import os\n",
    "import arviz as az\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import functools\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import json\n",
    "import gc\n",
    "import psutil\n",
    "import sys\n",
    "import threading\n",
    "import dataclasses\n",
    "import xarray as xr\n",
    "from pymc import *\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## 1.2.2 Unit Test for Imports\n",
    "try:\n",
    "    _ = [pm, tf, pd, np, plt, sns, stats, bigquery]\n",
    "    print(\"All libraries imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"Library import failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sswE7pNUt0xD",
   "metadata": {
    "id": "sswE7pNUt0xD"
   },
   "outputs": [],
   "source": [
    "# 1.2.3 Retrieve the list of installed and imported packages and their point versions\n",
    "# Install a specific point version of pymc\n",
    "# !pip install pymc==3.11.4\n",
    "# installed_packages = pkg_resources.working_set\n",
    "# package_versions = {package.key: package.version for package in installed_packages}\n",
    "# print(f\"Installed packages and versions:\\n{package_versions}\")\n",
    "%pip freeze > requirements.txt\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "to6_eahxY2yq",
   "metadata": {
    "id": "to6_eahxY2yq"
   },
   "outputs": [],
   "source": [
    "# 1.2.4 Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9I_YPRV56XIe",
   "metadata": {
    "id": "9I_YPRV56XIe"
   },
   "outputs": [],
   "source": [
    "# 1.2.7 Set numpyro to use available GPUs\n",
    "numpyro.set_host_device_count(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "RR7kvNJ-77qE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RR7kvNJ-77qE",
    "outputId": "347be3c2-43c9-47f5-e7c7-735122aa851c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JAX version: 0.4.35\n",
      "\n",
      "Available devices:\n",
      "[CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3)]\n"
     ]
    }
   ],
   "source": [
    "# 1.2.6 Number of GPUs discovered by jax\n",
    "import jax\n",
    "print(\"\\nJAX version:\", jax.__version__)\n",
    "print(\"\\nAvailable devices:\")\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Qv8WYTDZ6VD0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qv8WYTDZ6VD0",
    "outputId": "88b95b89-9110-414d-8963-ed12ac3b9a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-smi in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nvidia-smi) (1.26.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-smi) (1.16.0)\n",
      "Requirement already satisfied: sorcery>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-smi) (0.2.2)\n",
      "Requirement already satisfied: pytest>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from nvidia-smi) (7.4.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=4.3.1->nvidia-smi) (2.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest>=4.3.1->nvidia-smi) (24.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=4.3.1->nvidia-smi) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=4.3.1->nvidia-smi) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=4.3.1->nvidia-smi) (2.0.2)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.10/dist-packages (from sorcery>=0.1.0->nvidia-smi) (2.1.0)\n",
      "Requirement already satisfied: littleutils>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from sorcery>=0.1.0->nvidia-smi) (0.2.4)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.10/dist-packages (from sorcery>=0.1.0->nvidia-smi) (2.4.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from sorcery>=0.1.0->nvidia-smi) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Fri Nov 15 00:50:47 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   65C    P0             30W /   72W |     195MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   62C    P0             31W /   72W |     195MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA L4                      Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   61C    P0             31W /   72W |     195MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA L4                      Off |   00000000:00:06.0 Off |                    0 |\n",
      "| N/A   63C    P0             30W /   72W |     195MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 1.2.8 View GPU specs\n",
    "!pip install nvidia-smi torch\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XpdYViJyfevQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpdYViJyfevQ",
    "outputId": "96658520-ace1-4bdc-82df-24e999785b3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MIN_FREQUENCY': 1,\n",
       " 'MIN_REVENUE': 1,\n",
       " 'MIN_TRANSACTION_VALUE': 1,\n",
       " 'OUTLIER_THRESHOLD': 3,\n",
       " 'PROJECT_ID': 'logic-dna-240402',\n",
       " 'DATASET': 'CLV',\n",
       " 'TABLE': 'T_CLV_360',\n",
       " 'LIMIT': 1500000,\n",
       " 'COHORT_MONTH': '2019-02-01',\n",
       " 'MIN_PURCHASE_DATE': '2022-02-03',\n",
       " 'MAX_PURCHASE_DATE': None,\n",
       " 'INCLUDE_ONLINE': True,\n",
       " 'INCLUDE_STORE': True,\n",
       " 'MIN_LOYALTY_POINTS': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data processor config override\n",
    "dp_config = {\n",
    "  # Data processing parameters\n",
    "  'MIN_FREQUENCY': 1,\n",
    "  'MIN_REVENUE': 1,\n",
    "  'MIN_TRANSACTION_VALUE': 1,\n",
    "  'OUTLIER_THRESHOLD': 3,  # Number of IQRs for outlier detection\n",
    "  # Query parameters\n",
    "  'PROJECT_ID': 'logic-dna-240402',\n",
    "  'DATASET': 'CLV',\n",
    "  'TABLE': 'T_CLV_360',\n",
    "  'LIMIT': 1500000,\n",
    "  'COHORT_MONTH': '2019-02-01',\n",
    "  'MIN_PURCHASE_DATE': '2022-02-03',\n",
    "  'MAX_PURCHASE_DATE': None,  # Defaults to current date if None\n",
    "  'INCLUDE_ONLINE': True,\n",
    "  'INCLUDE_STORE': True,\n",
    "  'MIN_LOYALTY_POINTS': 0\n",
    "}\n",
    "dp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vDefWYEfsps1",
   "metadata": {
    "id": "vDefWYEfsps1"
   },
   "outputs": [],
   "source": [
    "# Data processor\n",
    "class CLVDataProcessor:\n",
    "    \"\"\"\n",
    "    CLV data processing class with data quality checks\n",
    "    \"\"\"\n",
    "    def __init__(self, config=None):\n",
    "        self.config = dp_config or {\n",
    "    # Data processing parameters\n",
    "    'MIN_FREQUENCY': 1,\n",
    "    'MIN_REVENUE': 1,\n",
    "    'MIN_TRANSACTION_VALUE': 1,\n",
    "    'OUTLIER_THRESHOLD': 3,  # Number of IQRs for outlier detection\n",
    "    # Query parameters\n",
    "    'PROJECT_ID': 'logic-dna-240402',\n",
    "    'DATASET': 'CLV',\n",
    "    'TABLE': 'T_CLV_360',\n",
    "    'LIMIT': 10000000,\n",
    "    'COHORT_MONTH': '2023-02-01',\n",
    "    'MIN_PURCHASE_DATE': '2022-02-03',\n",
    "    'MAX_PURCHASE_DATE': 'CURRENT_DATE()',  # Defaults to current date if None\n",
    "    'INCLUDE_ONLINE': True,\n",
    "    'INCLUDE_STORE': True,\n",
    "    'MIN_LOYALTY_POINTS': 0\n",
    "  }\n",
    "        self.data = None\n",
    "        self.quality_flags = None\n",
    "\n",
    "    def _build_query(self):\n",
    "        \"\"\"Build BigQuery query based on configuration parameters\"\"\"\n",
    "        # Handle date parameters\n",
    "        max_date = self.config['MAX_PURCHASE_DATE'] or 'CURRENT_DATE()'\n",
    "        min_date = f\"DATE('{self.config['MIN_PURCHASE_DATE']}')\"\n",
    "        cohort_month = f\"DATE('{self.config['COHORT_MONTH']}')\"\n",
    "\n",
    "        # Build channel condition\n",
    "        channel_conditions = []\n",
    "        if self.config['INCLUDE_ONLINE']:\n",
    "            channel_conditions.append(\"has_online_purchases = 1\")\n",
    "        if self.config['INCLUDE_STORE']:\n",
    "            channel_conditions.append(\"has_store_purchases = 1\")\n",
    "        channel_filter = f\"({' OR '.join(channel_conditions)})\" if channel_conditions else \"TRUE\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        WITH\n",
    "        fin AS (\n",
    "        SELECT\n",
    "          CAST(customer_id AS STRING) AS customer_id,\n",
    "          CAST(cohort_month AS STRING) AS cohort_month,\n",
    "          CAST(recency_days AS INT64) AS recency,\n",
    "          CAST(frequency AS INT64) AS frequency,\n",
    "          ROUND(total_revenue,2) AS monetary,\n",
    "          ROUND(total_revenue,2) AS total_revenue,\n",
    "          ROUND(revenue_trend,4) AS revenue_trend,\n",
    "          ROUND(avg_transaction_value,2) AS avg_transaction_value,\n",
    "          CAST(first_purchase_date AS DATE) AS first_purchase_date,\n",
    "          CAST(last_purchase_date AS DATE) AS last_purchase_date,\n",
    "          CAST(customer_age_days AS INT64) AS customer_age_days,\n",
    "          CAST(distinct_categories AS INT64) AS distinct_categories,\n",
    "          CAST(distinct_brands AS INT64) AS distinct_brands,\n",
    "          ROUND(avg_interpurchase_days,2) AS avg_interpurchase_days,\n",
    "          CAST(has_online_purchases AS INT64) AS has_online_purchases,\n",
    "          CAST(has_store_purchases AS INT64) AS has_store_purchases,\n",
    "          ROUND(total_discount_amount,2) AS total_discount_amount,\n",
    "          ROUND(avg_discount_amount,2) AS avg_discount_amount,\n",
    "          ROUND(COALESCE(discount_rate,0),3) AS discount_rate,\n",
    "          CAST(sms_active AS INT64) AS sms_active,\n",
    "          CAST(email_active AS INT64) AS email_active,\n",
    "          CAST(is_loyalty_member AS INT64) AS is_loyalty_member,\n",
    "          CAST(loyalty_points AS INT64) AS loyalty_points\n",
    "        FROM\n",
    "          `{self.config['PROJECT_ID']}.{self.config['DATASET']}.{self.config['TABLE']}`\n",
    "        WHERE\n",
    "          customer_id IS NOT NULL\n",
    "          AND cohort_month IS NOT NULL\n",
    "          AND frequency >= {self.config['MIN_FREQUENCY']}\n",
    "          AND total_revenue >= {self.config['MIN_REVENUE']}\n",
    "          AND avg_transaction_value >= {self.config['MIN_TRANSACTION_VALUE']}\n",
    "          AND cohort_month >= {cohort_month}\n",
    "          # AND first_purchase_date >= {min_date}\n",
    "          AND last_purchase_date <= {max_date}\n",
    "          AND loyalty_points >= {self.config['MIN_LOYALTY_POINTS']}\n",
    "          AND {channel_filter}\n",
    "        )\n",
    "        SELECT\n",
    "          *\n",
    "        FROM\n",
    "          fin\n",
    "        LIMIT\n",
    "          {self.config['LIMIT']}\n",
    "        \"\"\"\n",
    "\n",
    "        return query\n",
    "\n",
    "    def load_data(self, query=None, project_id=None, csv_path=None):\n",
    "        \"\"\"Load data either from BigQuery or CSV file\"\"\"\n",
    "        try:\n",
    "            if csv_path:\n",
    "                self.data = pd.read_csv(csv_path)\n",
    "                print(f\"Successfully loaded {len(self.data):,} records from CSV\")\n",
    "            else:\n",
    "                from google.cloud import bigquery\n",
    "\n",
    "                client = bigquery.Client(project=project_id or self.config['PROJECT_ID'])\n",
    "\n",
    "                # Build the query using config parameters\n",
    "                default_query = self._build_query()\n",
    "\n",
    "                self.data = client.query(query or default_query).to_dataframe()\n",
    "                print(f\"Successfully loaded {len(self.data):,} records from BigQuery\")\n",
    "\n",
    "            # Convert date columns\n",
    "            date_columns = ['first_purchase_date', 'last_purchase_date', 'cohort_month']\n",
    "            for col in date_columns:\n",
    "                if col in self.data.columns:\n",
    "                    self.data[col] = pd.to_datetime(self.data[col])\n",
    "\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Main data processing pipeline that handles:\n",
    "        - Basic cleaning\n",
    "        - RFM calculation\n",
    "        - Quality validation\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Starting data processing. Initial shape: {self.data.shape}\")\n",
    "\n",
    "            # 1. Basic cleaning\n",
    "            self._clean_basic_data()\n",
    "\n",
    "            # 2. Calculate RFM metrics\n",
    "            self._calculate_rfm_metrics()\n",
    "\n",
    "            # 3. Handle outliers and invalid values\n",
    "            self._clean_monetary_values()\n",
    "            self._clean_categorical_features()\n",
    "\n",
    "            # 4. Validate data quality\n",
    "            self._validate_data_quality()\n",
    "\n",
    "            # 5. Generate final report\n",
    "            self._generate_quality_report()\n",
    "\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in data processing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _clean_basic_data(self):\n",
    "        \"\"\"Basic data cleaning operations\"\"\"\n",
    "        print(\"Cleaning data...\")\n",
    "        initial_count = len(self.data)\n",
    "\n",
    "        # Remove invalid records\n",
    "        self.data = self.data[\n",
    "            (self.data['frequency'] >= self.config['MIN_FREQUENCY']) &\n",
    "            (self.data['total_revenue'] > 0) &\n",
    "            (self.data['avg_transaction_value'] > 0)\n",
    "        ]\n",
    "\n",
    "        # Drop duplicates\n",
    "        self.data = self.data.drop_duplicates(subset=['customer_id'])\n",
    "\n",
    "        # Handle missing values\n",
    "        self.data['discount_rate'] = self.data['discount_rate'].fillna(0)\n",
    "\n",
    "        records_removed = initial_count - len(self.data)\n",
    "        print(f\"Records removed: {records_removed}\")\n",
    "\n",
    "    def _calculate_rfm_metrics(self):\n",
    "        \"\"\"Calculate Recency, Frequency, Monetary metrics\"\"\"\n",
    "        current_date = pd.Timestamp.today()\n",
    "\n",
    "        # Calculate or update RFM metrics\n",
    "        if 'recency' not in self.data.columns:\n",
    "            self.data['recency'] = (\n",
    "                current_date - self.data['last_purchase_date']\n",
    "            ).dt.days\n",
    "\n",
    "        if 'customer_age_days' not in self.data.columns:\n",
    "            self.data['customer_age_days'] = (\n",
    "                current_date - self.data['first_purchase_date']\n",
    "            ).dt.days\n",
    "\n",
    "        if 'avg_transaction_value' not in self.data.columns:\n",
    "            self.data['avg_transaction_value'] = (\n",
    "                self.data['total_revenue'] / self.data['frequency']\n",
    "            )\n",
    "\n",
    "    def _clean_monetary_values(self):\n",
    "        \"\"\"Handle monetary value cleaning and outliers\"\"\"\n",
    "        for col in ['frequency', 'monetary', 'avg_transaction_value', 'total_revenue']:\n",
    "            if col in self.data.columns:\n",
    "                # Remove negative values\n",
    "                self.data = self.data[self.data[col] >= 0]\n",
    "\n",
    "                # Handle outliers using IQR method\n",
    "                Q1 = self.data[col].quantile(0.25)\n",
    "                Q3 = self.data[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 3 * IQR\n",
    "                upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "                self.data = self.data[\n",
    "                    (self.data[col] >= lower_bound) &\n",
    "                    (self.data[col] <= upper_bound)\n",
    "                ]\n",
    "\n",
    "    def _clean_categorical_features(self):\n",
    "        \"\"\"Clean and encode categorical features\"\"\"\n",
    "        categorical_features = {\n",
    "            'has_online_purchases': 0,\n",
    "            'has_store_purchases': 0,\n",
    "            'is_loyalty_member': 0,\n",
    "            'loyalty_points': 0,\n",
    "            'sms_active': 0,\n",
    "            'email_active': 0\n",
    "        }\n",
    "\n",
    "        for col, default in categorical_features.items():\n",
    "            if col in self.data.columns:\n",
    "                self.data[col] = self.data[col].fillna(default).astype(int)\n",
    "\n",
    "    def _validate_data_quality(self):\n",
    "        \"\"\"Validate data quality and create quality flags\"\"\"\n",
    "        # Create quality flags\n",
    "        self.quality_flags = pd.DataFrame(index=self.data.index)\n",
    "\n",
    "        # Define validation rules\n",
    "        validations = {\n",
    "            'valid_frequency': self.data['frequency'] >= self.config['MIN_FREQUENCY'],\n",
    "            'valid_recency': self.data['recency'] >= 0,\n",
    "            'valid_monetary': self.data['avg_transaction_value'] > 0,\n",
    "            'valid_dates': self.data['last_purchase_date'] >= self.data['first_purchase_date']\n",
    "        }\n",
    "\n",
    "        # Apply validations\n",
    "        for flag_name, condition in validations.items():\n",
    "            self.quality_flags[flag_name] = condition\n",
    "\n",
    "        # Overall validation\n",
    "        self.quality_flags['overall_valid'] = self.quality_flags.all(axis=1)\n",
    "\n",
    "    def _generate_quality_report(self):\n",
    "        \"\"\"Generate final data quality report\"\"\"\n",
    "        report = {\n",
    "            'record_count': len(self.data),\n",
    "            'metrics': {\n",
    "                'frequency_mean': self.data['frequency'].mean(),\n",
    "                'recency_mean': self.data['recency'].mean(),\n",
    "                'monetary_mean': self.data['avg_transaction_value'].mean()\n",
    "            },\n",
    "            'quality_flags': {\n",
    "                col: self.quality_flags[col].mean() * 100\n",
    "                for col in self.quality_flags.columns\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\nQuality Report:\")\n",
    "        print(f\"Records processed: {report['record_count']:,}\")\n",
    "        print(\"\\nKey Metrics:\")\n",
    "        for metric, value in report['metrics'].items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "        print(\"\\nQuality Flags (% passing):\")\n",
    "        for flag, pct in report['quality_flags'].items():\n",
    "            print(f\"{flag}: {pct:.1f}%\")\n",
    "\n",
    "    def get_processed_data(self):\n",
    "        \"\"\"Return the processed DataFrame\"\"\"\n",
    "        return self.data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qXATiS4Fsvh-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXATiS4Fsvh-",
    "outputId": "62111ccd-453b-45e8-842c-ef7f010070ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1,409,887 records from BigQuery\n",
      "Starting data processing. Initial shape: (1409887, 23)\n",
      "Cleaning data...\n",
      "Records removed: 0\n",
      "\n",
      "Quality Report:\n",
      "Records processed: 1,200,823\n",
      "\n",
      "Key Metrics:\n",
      "frequency_mean: 1.92\n",
      "recency_mean: 855.97\n",
      "monetary_mean: 43.45\n",
      "\n",
      "Quality Flags (% passing):\n",
      "valid_frequency: 100.0%\n",
      "valid_recency: 100.0%\n",
      "valid_monetary: 100.0%\n",
      "valid_dates: 100.0%\n",
      "overall_valid: 100.0%\n",
      "\n",
      "First few rows of the loaded data:\n",
      "                               customer_id cohort_month  recency  frequency  \\\n",
      "3567  95e48406-b3f7-3d04-94a7-9c1a8759597e   2024-02-01      256          1   \n",
      "3568  c9e404f3-3b9b-3731-ac35-b7fca2506e96   2024-02-01      256          1   \n",
      "3569  6909ef74-cf90-374b-833e-0ec339c61050   2024-02-01      256          1   \n",
      "3570  26794b35-c9e6-3346-a848-ee97599fd568   2024-02-01      256          1   \n",
      "3571  039f159b-d60f-35ac-8df0-4e6c2100defb   2024-02-01      256          1   \n",
      "\n",
      "      monetary  total_revenue  revenue_trend  avg_transaction_value  \\\n",
      "3567     16.00          16.00            0.0                   5.33   \n",
      "3568     48.00          48.00            0.0                  48.00   \n",
      "3569    174.00         174.00            0.0                  43.50   \n",
      "3570     23.22          23.22            0.0                   5.81   \n",
      "3571     42.00          42.00            0.0                  14.00   \n",
      "\n",
      "     first_purchase_date last_purchase_date  ...  avg_interpurchase_days  \\\n",
      "3567          2024-02-28         2024-02-28  ...                     0.0   \n",
      "3568          2024-02-28         2024-02-28  ...                     0.0   \n",
      "3569          2024-02-28         2024-02-28  ...                     0.0   \n",
      "3570          2024-02-28         2024-02-28  ...                     0.0   \n",
      "3571          2024-02-28         2024-02-28  ...                     0.0   \n",
      "\n",
      "      has_online_purchases  has_store_purchases  total_discount_amount  \\\n",
      "3567                     1                    0                   0.00   \n",
      "3568                     0                    1                   0.00   \n",
      "3569                     0                    1                   0.00   \n",
      "3570                     1                    0                   2.58   \n",
      "3571                     1                    0                   0.00   \n",
      "\n",
      "      avg_discount_amount  discount_rate  sms_active  email_active  \\\n",
      "3567                 0.00            0.0           0             1   \n",
      "3568                 0.00            0.0           0             1   \n",
      "3569                 0.00            0.0           0             0   \n",
      "3570                 0.65            0.1           0             0   \n",
      "3571                 0.00            0.0           0             0   \n",
      "\n",
      "      is_loyalty_member  loyalty_points  \n",
      "3567                  0               0  \n",
      "3568                  1             480  \n",
      "3569                  0               0  \n",
      "3570                  1             232  \n",
      "3571                  1             420  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Dataframe information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1200823 entries, 3567 to 1403249\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count    Dtype         \n",
      "---  ------                  --------------    -----         \n",
      " 0   customer_id             1200823 non-null  object        \n",
      " 1   cohort_month            1200823 non-null  datetime64[ns]\n",
      " 2   recency                 1200823 non-null  Int64         \n",
      " 3   frequency               1200823 non-null  Int64         \n",
      " 4   monetary                1200823 non-null  float64       \n",
      " 5   total_revenue           1200823 non-null  float64       \n",
      " 6   revenue_trend           1200823 non-null  float64       \n",
      " 7   avg_transaction_value   1200823 non-null  float64       \n",
      " 8   first_purchase_date     1200823 non-null  datetime64[ns]\n",
      " 9   last_purchase_date      1200823 non-null  datetime64[ns]\n",
      " 10  customer_age_days       1200823 non-null  Int64         \n",
      " 11  distinct_categories     1200823 non-null  Int64         \n",
      " 12  distinct_brands         1200823 non-null  Int64         \n",
      " 13  avg_interpurchase_days  1200823 non-null  float64       \n",
      " 14  has_online_purchases    1200823 non-null  int64         \n",
      " 15  has_store_purchases     1200823 non-null  int64         \n",
      " 16  total_discount_amount   1200823 non-null  float64       \n",
      " 17  avg_discount_amount     1200823 non-null  float64       \n",
      " 18  discount_rate           1200823 non-null  float64       \n",
      " 19  sms_active              1200823 non-null  int64         \n",
      " 20  email_active            1200823 non-null  int64         \n",
      " 21  is_loyalty_member       1200823 non-null  int64         \n",
      " 22  loyalty_points          1200823 non-null  int64         \n",
      "dtypes: Int64(5), datetime64[ns](3), float64(8), int64(6), object(1)\n",
      "memory usage: 225.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create processor instance\n",
    "processor = CLVDataProcessor(dp_config)\n",
    "\n",
    "# Load the query with parameters\n",
    "processor._build_query()\n",
    "\n",
    "# Load your DataFrame\n",
    "processor.load_data()\n",
    "\n",
    "# Process the data\n",
    "processor.process_data()\n",
    "\n",
    "# Get the processed data\n",
    "processed_df = processor.get_processed_data()\n",
    "\n",
    "# View first few rows\n",
    "print(\"\\nFirst few rows of the loaded data:\")\n",
    "print(processed_df.head())\n",
    "\n",
    "# Display dataframe info\n",
    "print(\"\\nDataframe information:\")\n",
    "print(processed_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3vmD6xgQS1ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3vmD6xgQS1ca",
    "outputId": "beed872b-04db-40aa-fbac-f0cfdc04efa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         customer_index                           customer_id\n",
      "0                     0  95e48406-b3f7-3d04-94a7-9c1a8759597e\n",
      "1                     1  c9e404f3-3b9b-3731-ac35-b7fca2506e96\n",
      "2                     2  6909ef74-cf90-374b-833e-0ec339c61050\n",
      "3                     3  26794b35-c9e6-3346-a848-ee97599fd568\n",
      "4                     4  039f159b-d60f-35ac-8df0-4e6c2100defb\n",
      "...                 ...                                   ...\n",
      "1200818         1200818  4168e105-5a45-3caa-a67a-2d34db664364\n",
      "1200819         1200819  07979f36-5756-3e1c-8a43-d87b5517815c\n",
      "1200820         1200820  3fe1ec23-bd3e-3dde-b3d2-759f75b469ef\n",
      "1200821         1200821  2943023b-a840-301b-9829-29fee52bee13\n",
      "1200822         1200822  2af337d4-fb15-3a22-ae5e-6c97b347ab21\n",
      "\n",
      "[1200823 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200823 entries, 0 to 1200822\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count    Dtype         \n",
      "---  ------                  --------------    -----         \n",
      " 0   cohort_month            1200823 non-null  datetime64[ns]\n",
      " 1   recency                 1200823 non-null  Int64         \n",
      " 2   frequency               1200823 non-null  Int64         \n",
      " 3   monetary                1200823 non-null  float64       \n",
      " 4   total_revenue           1200823 non-null  float64       \n",
      " 5   revenue_trend           1200823 non-null  float64       \n",
      " 6   avg_transaction_value   1200823 non-null  float64       \n",
      " 7   first_purchase_date     1200823 non-null  datetime64[ns]\n",
      " 8   last_purchase_date      1200823 non-null  datetime64[ns]\n",
      " 9   customer_age_days       1200823 non-null  Int64         \n",
      " 10  distinct_categories     1200823 non-null  Int64         \n",
      " 11  distinct_brands         1200823 non-null  Int64         \n",
      " 12  avg_interpurchase_days  1200823 non-null  float64       \n",
      " 13  has_online_purchases    1200823 non-null  int64         \n",
      " 14  has_store_purchases     1200823 non-null  int64         \n",
      " 15  total_discount_amount   1200823 non-null  float64       \n",
      " 16  avg_discount_amount     1200823 non-null  float64       \n",
      " 17  discount_rate           1200823 non-null  float64       \n",
      " 18  sms_active              1200823 non-null  int64         \n",
      " 19  email_active            1200823 non-null  int64         \n",
      " 20  is_loyalty_member       1200823 non-null  int64         \n",
      " 21  loyalty_points          1200823 non-null  int64         \n",
      "dtypes: Int64(5), datetime64[ns](3), float64(8), int64(6)\n",
      "memory usage: 207.3 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-bc24c985-ae9f-4fb9-81fa-1501104e567f\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_month</th>\n",
       "      <th>recency</th>\n",
       "      <th>frequency</th>\n",
       "      <th>monetary</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>revenue_trend</th>\n",
       "      <th>avg_transaction_value</th>\n",
       "      <th>first_purchase_date</th>\n",
       "      <th>last_purchase_date</th>\n",
       "      <th>customer_age_days</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_interpurchase_days</th>\n",
       "      <th>has_online_purchases</th>\n",
       "      <th>has_store_purchases</th>\n",
       "      <th>total_discount_amount</th>\n",
       "      <th>avg_discount_amount</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>sms_active</th>\n",
       "      <th>email_active</th>\n",
       "      <th>is_loyalty_member</th>\n",
       "      <th>loyalty_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>16.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>48.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>174.00</td>\n",
       "      <td>174.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.50</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>23.22</td>\n",
       "      <td>23.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.81</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>42.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc24c985-ae9f-4fb9-81fa-1501104e567f')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-bc24c985-ae9f-4fb9-81fa-1501104e567f button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-bc24c985-ae9f-4fb9-81fa-1501104e567f');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1dd256f5-5246-4cf6-b5b6-b855f8f4db78\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1dd256f5-5246-4cf6-b5b6-b855f8f4db78')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1dd256f5-5246-4cf6-b5b6-b855f8f4db78 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  cohort_month  recency  frequency  monetary  total_revenue  revenue_trend  \\\n",
       "0   2024-02-01      256          1     16.00          16.00            0.0   \n",
       "1   2024-02-01      256          1     48.00          48.00            0.0   \n",
       "2   2024-02-01      256          1    174.00         174.00            0.0   \n",
       "3   2024-02-01      256          1     23.22          23.22            0.0   \n",
       "4   2024-02-01      256          1     42.00          42.00            0.0   \n",
       "\n",
       "   avg_transaction_value first_purchase_date last_purchase_date  \\\n",
       "0                   5.33          2024-02-28         2024-02-28   \n",
       "1                  48.00          2024-02-28         2024-02-28   \n",
       "2                  43.50          2024-02-28         2024-02-28   \n",
       "3                   5.81          2024-02-28         2024-02-28   \n",
       "4                  14.00          2024-02-28         2024-02-28   \n",
       "\n",
       "   customer_age_days  ...  avg_interpurchase_days  has_online_purchases  \\\n",
       "0                256  ...                     0.0                     1   \n",
       "1                256  ...                     0.0                     0   \n",
       "2                256  ...                     0.0                     0   \n",
       "3                256  ...                     0.0                     1   \n",
       "4                256  ...                     0.0                     1   \n",
       "\n",
       "   has_store_purchases  total_discount_amount  avg_discount_amount  \\\n",
       "0                    0                   0.00                 0.00   \n",
       "1                    1                   0.00                 0.00   \n",
       "2                    1                   0.00                 0.00   \n",
       "3                    0                   2.58                 0.65   \n",
       "4                    0                   0.00                 0.00   \n",
       "\n",
       "   discount_rate  sms_active  email_active  is_loyalty_member  loyalty_points  \n",
       "0            0.0           0             1                  0               0  \n",
       "1            0.0           0             1                  1             480  \n",
       "2            0.0           0             0                  0               0  \n",
       "3            0.1           0             0                  1             232  \n",
       "4            0.0           0             0                  1             420  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-index the processed_df and create a customer_index column\n",
    "processed_df.reset_index(drop=True, inplace=True)\n",
    "processed_df['customer_index'] = processed_df.index\n",
    "\n",
    "# Create a new DataFrame with the specified columns\n",
    "customer_lookup_df = processed_df[['customer_index', 'customer_id']].copy()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(customer_lookup_df)\n",
    "\n",
    "# Drop the 'customer_id' field from processed_df\n",
    "processed_df = processed_df.drop(columns=['customer_index', 'customer_id'])\n",
    "\n",
    "# Display the updated processed_df structure\n",
    "print(processed_df.info())\n",
    "\n",
    "customer_lookup_df.head()\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xpaD9_Lo16dC",
   "metadata": {
    "id": "xpaD9_Lo16dC"
   },
   "source": [
    "P(frequency, recency | parameters, covariates) = \\\n",
    "      Poisson(freq | exp(r + XÎ²_r) * T) Ã— \\\n",
    "      Exponential(rec | exp(Î± + XÎ²_Î±))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "EuC3X4pxNcxH",
   "metadata": {
    "id": "EuC3X4pxNcxH"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "import jax\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "class HierarchicalCLVSystem:\n",
    "    \"\"\"Hierarchical Customer Lifetime Value System\"\"\"\n",
    "  ## Class Attributes\n",
    "    # Configurable default segment configuration\n",
    "    default_segment_config = {\n",
    "        'use_rfm': True,              # RFM segmentation\n",
    "        'rfm_bins': {\n",
    "            'frequency': 4,           # Number of bins for frequency\n",
    "            'recency': 4,             # Number of bins for recency\n",
    "            'monetary': 4             # Number of bins for monetary\n",
    "        },\n",
    "\n",
    "        # Engagement Segmentation\n",
    "        'use_engagement': True,        # Engagement metrics\n",
    "        'engagement_bins': 4,          # Number of engagement level bins\n",
    "        'engagement_metrics': [        # Which metrics to use\n",
    "            'sms_active',\n",
    "            'email_active',\n",
    "            'is_loyalty_member'\n",
    "        ],\n",
    "\n",
    "        # Channel Segmentation\n",
    "        'use_channel': True,           # Channel preference\n",
    "\n",
    "        # Loyalty Segmentation\n",
    "        'use_loyalty': True,           # Loyalty segments\n",
    "        'loyalty_bins': 4,             # Number of loyalty level bins\n",
    "\n",
    "        # Cohort Segmentation\n",
    "        'use_cohorts': True,          # Cohort grouping\n",
    "        'cohort_type': 'quarterly',     # 'monthly' or 'quarterly'\n",
    "\n",
    "        # Combined Segmentation\n",
    "        'use_combined': True,         # Whether to create combined segments\n",
    "\n",
    "        # Segment Size Controls\n",
    "        'min_segment_size': 200,      # Minimum customers per segment\n",
    "        'merge_small_segments': True  # Whether to merge small segments into 'other'\n",
    "    }\n",
    "\n",
    "    # Core data requirements\n",
    "    required_columns = {\n",
    "        'transaction': [\n",
    "            'frequency', 'recency', 'customer_age_days',\n",
    "            'monetary', 'avg_transaction_value'\n",
    "        ],\n",
    "        'customer': [\n",
    "            'cohort_month', 'distinct_categories', 'distinct_brands',\n",
    "            'avg_interpurchase_days', 'has_online_purchases', 'has_store_purchases'\n",
    "        ],\n",
    "        'engagement': [\n",
    "            'sms_active', 'email_active', 'is_loyalty_member', 'loyalty_points'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    segment_configs = {\n",
    "        'transaction': {\n",
    "            'frequency': {\n",
    "                'n_bins': 4,\n",
    "                'labels': ['low', 'medium', 'high']\n",
    "            },\n",
    "            'recency': {\n",
    "                'n_bins': 4,\n",
    "                'labels': ['recent', 'mid', 'old']\n",
    "            },\n",
    "            'monetary': {\n",
    "                'n_bins': 4,\n",
    "                'labels': ['low', 'medium', 'high']\n",
    "            }\n",
    "        },\n",
    "        'engagement': {\n",
    "            'metrics': ['sms_active', 'email_active', 'is_loyalty_member'],\n",
    "            'loyalty_points': {\n",
    "                'n_bins': 4,\n",
    "                'labels': ['low', 'medium', 'high']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Initialization\n",
    "    def __init__(self, config: Dict[str, Any], segment_config: Optional[Dict] = None):\n",
    "        \"\"\"Initialize system with configuration\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : Dict[str, Any]\n",
    "            Configuration dictionary with all parameters\n",
    "        segment_config : Optional[Dict]\n",
    "            Optional separate segmentation config (falls back to config['segment_config'])\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.segment_config = segment_config or config.get('segment_config', {})\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.trace = None\n",
    "\n",
    "        # Initialize segmentation bins dictionary\n",
    "        self.segment_bins = {\n",
    "            'rfm': {},\n",
    "            'loyalty': {},\n",
    "            'engagement': {}\n",
    "        }\n",
    "\n",
    "        # Store medians for later use\n",
    "        self.medians = {}\n",
    "\n",
    "        # Initialize tracking\n",
    "        self.gpu_enabled = False\n",
    "        self.convergence_history = []\n",
    "        self.training_metrics = {\n",
    "            'n_divergent': 0,\n",
    "            'max_rhat': None,\n",
    "            'min_ess': None\n",
    "        }\n",
    "\n",
    "        # Setup GPU if enabled\n",
    "        if self.config.get('use_gpu', True):\n",
    "            self._setup_gpu()\n",
    "\n",
    "        # Setup monitoring if enabled\n",
    "        if self.config.get('monitor_resources', True):\n",
    "            self.setup_monitoring()\n",
    "\n",
    "    def run_analysis(self, processed_df=None, custom_config=None):\n",
    "        \"\"\"Run complete CLV analysis pipeline with progress monitoring\"\"\"\n",
    "        try:\n",
    "            print(\"\\n=== Starting CLV Analysis ===\")\n",
    "\n",
    "            if processed_df is None:\n",
    "                raise ValueError(\"No data provided\")\n",
    "\n",
    "            # Store original data\n",
    "            self.original_data = processed_df.copy()\n",
    "\n",
    "            # 1. Update configurations if provided\n",
    "            if custom_config:\n",
    "                print(\"\\n1. Updating Configuration...\")\n",
    "                self.config = self.config_manager.setup_config(custom_config)\n",
    "\n",
    "            # 2. Pre-process data and create Segments\n",
    "            try:\n",
    "                print(\"\\n2.1 Processing Data...\")\n",
    "                # Print initial data info\n",
    "                print(f\"Initial shape: {processed_df.shape}\")\n",
    "                print(\"Columns:\", processed_df.columns.tolist())\n",
    "\n",
    "                # Verify and preprocess\n",
    "                self._verify_model_columns(processed_df)\n",
    "                processed_data = self._preprocess_data(processed_df)\n",
    "                if processed_data is None:\n",
    "                    raise ValueError(\"Data preprocessing failed\")\n",
    "\n",
    "                print(\"\\n2.2 Creating Segments...\")\n",
    "                segmented_data = self.create_segments(processed_data)\n",
    "                if segmented_data is None:\n",
    "                    raise ValueError(\"Segmentation failed\")\n",
    "\n",
    "                # Print segmentation summary\n",
    "                print(\"\\nSegmentation Summary:\")\n",
    "                for col in ['rfm_segment', 'engagement_level', 'loyalty_segment', 'cohort_segment']:\n",
    "                    if col in segmented_data.columns:\n",
    "                        print(f\"\\n{col} distribution:\")\n",
    "                        print(segmented_data[col].value_counts().head())\n",
    "\n",
    "                # Sample N records from the segmented data\n",
    "                sample_size = self.config.get('sample_size', 25000)\n",
    "                sample_size = min(sample_size, len(segmented_data))\n",
    "                sampled_data = segmented_data.sample(n=sample_size, random_state=42)\n",
    "                sampled_data_shape = sampled_data.shape\n",
    "                num_columns = min(10, sampled_data.shape[1])\n",
    "                columns_to_sort = sampled_data.columns[:num_columns]\n",
    "                sorted_data = sampled_data.sort_values(by=list(columns_to_sort))\n",
    "                self.data = sorted_data\n",
    "\n",
    "                # Save the sorted DataFrame to a CSV file (Optional)\n",
    "                file_path = '/content/sample_sorted_data.csv'\n",
    "                sorted_data.to_csv(file_path, index=True)\n",
    "\n",
    "                print(f\"\\nSampled {sample_size} records from segmented data\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Data processing and segmentation error: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "            # 3. Batch Data Processing\n",
    "            try:\n",
    "                # Process in batches\n",
    "                print(\"\\n3. Processing Batches...\")\n",
    "                self.data = self.process_batches(self.data)\n",
    "                if self.data is None:\n",
    "                    raise ValueError(\"Batch processing failed\")\n",
    "\n",
    "                # Print data quality metrics\n",
    "                print(\"\\nProcessed Data Summary:\")\n",
    "                print(f\"Final shape: {self.data.shape}\")\n",
    "                print(\"\\nMissing values:\")\n",
    "                print(self.data.isnull().sum()[self.data.isnull().sum() > 0])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Batch processing error: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "            # 4. Build and Train Model\n",
    "            try:\n",
    "                print(\"\\n5. Building Model...\")\n",
    "                self.build_model()\n",
    "                if self.model is None:\n",
    "                    raise ValueError(\"Model building failed\")\n",
    "\n",
    "                print(\"\\n6. Training Model...\")\n",
    "                # Setup GPU if enabled\n",
    "                if self.config.get('use_gpu', False):\n",
    "                    print(\"\\nOptimizing GPU resources...\")\n",
    "                    self._setup_gpu()\n",
    "                    self.optimize_gpu_memory()\n",
    "\n",
    "                # Train model\n",
    "                self.train_model()\n",
    "                if self.trace is None:\n",
    "                    raise ValueError(\"Model training failed\")\n",
    "\n",
    "                # Check convergence\n",
    "                convergence_ok = self._check_convergence()\n",
    "                if not convergence_ok:\n",
    "                    print(\"\\nWarning: Model may not have converged properly\")\n",
    "                    print(\"Consider adjusting parameters or reducing model complexity\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Model building/training error: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "            # 5. Generate Results\n",
    "            try:\n",
    "                print(\"\\n7. Generating Results...\")\n",
    "                results = self._generate_results()\n",
    "                if results is None:\n",
    "                    raise ValueError(\"Results generation failed\")\n",
    "\n",
    "                # Print key metrics\n",
    "                print(\"\\nKey Model Metrics:\")\n",
    "                if 'metrics' in results:\n",
    "                    for metric, value in results['metrics'].items():\n",
    "                        if isinstance(value, dict):  # Handle nested dictionaries\n",
    "                            print(f\"\\n{metric}:\")\n",
    "                            for k, v in value.items():\n",
    "                                if isinstance(v, (int, float)):\n",
    "                                    print(f\"  {k}: {v:.4f}\")  # Format as float\n",
    "                                else:\n",
    "                                    print(f\"  {k}: {v}\")  # Print as string\n",
    "                        else:\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                print(f\"\\n{metric}: {value:.4f}\")  # Format as float\n",
    "                            else:\n",
    "                                print(f\"\\n{metric}: {value}\")  # Print as string\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Results generation error: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "            # 6. Save Model\n",
    "            try:\n",
    "                print(\"\\n8. Saving Model...\")\n",
    "                # Check model status before saving\n",
    "                is_ready, message = self.check_model_status(self)\n",
    "                if is_ready:\n",
    "                    saved_path = self.save_trained_model(prefix='clv_model')\n",
    "                    print(f\"Model saved to: {saved_path}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Model not saved - {message}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Model saving error: {str(e)}\")\n",
    "                print(\"Continuing without saving...\")\n",
    "\n",
    "            print(\"\\n=== Analysis Complete ===\")\n",
    "            return self, results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAnalysis pipeline error: {str(e)}\")\n",
    "            import traceback\n",
    "            print(\"\\nDetailed error:\")\n",
    "            print(traceback.format_exc())\n",
    "            return None, None\n",
    "\n",
    "    # Check model status before trying to save:\n",
    "    @staticmethod\n",
    "    def check_model_status(model):\n",
    "        \"\"\"Check if model is properly trained and ready to save\"\"\"\n",
    "        if model is None:\n",
    "            return False, \"Model is None\"\n",
    "\n",
    "        if not hasattr(model, 'trace') or model.trace is None:\n",
    "            return False, \"No trace available - model may not have trained successfully\"\n",
    "\n",
    "        if not hasattr(model, 'data') or model.data is None:\n",
    "            return False, \"No data available - model may not have processed data\"\n",
    "\n",
    "        return True, \"Model ready to save\"\n",
    "\n",
    "    # 2. Data processing methods\n",
    "\n",
    "    def _preprocess_data(self, processed_df):\n",
    "        \"\"\"Preprocess data with proper type conversion\"\"\"\n",
    "        try:\n",
    "\n",
    "            df = processed_df.copy()\n",
    "\n",
    "            # Convert data types\n",
    "            numeric_cols = (\n",
    "                self.required_columns['transaction'] +\n",
    "                ['distinct_categories', 'distinct_brands', 'loyalty_points']\n",
    "            )\n",
    "\n",
    "            bool_cols = (\n",
    "                self.required_columns['engagement'][:-1] +  # Exclude loyalty_points\n",
    "                ['has_online_purchases', 'has_store_purchases']\n",
    "            )\n",
    "\n",
    "            # Convert numeric columns\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "                    # Integer conversion for count columns\n",
    "                    if col in ['frequency', 'distinct_categories', 'distinct_brands', 'loyalty_points']:\n",
    "                        df[col] = df[col].fillna(0).astype('int32')\n",
    "                    else:\n",
    "                        df[col] = df[col].fillna(0).astype('float32')\n",
    "\n",
    "            # Convert boolean columns\n",
    "            for col in bool_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype('int32')\n",
    "\n",
    "            # Convert dates\n",
    "            df['cohort_month'] = pd.to_datetime(df['cohort_month'])\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _verify_model_columns(self, processed_df):\n",
    "        \"\"\"Verify all required columns exist in DataFrame\"\"\"\n",
    "        missing_cols = []\n",
    "        for category, cols in self.required_columns.items():\n",
    "            missing = [col for col in cols if col not in processed_df.columns]\n",
    "            if missing:\n",
    "                missing_cols.extend(missing)\n",
    "\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    def create_segments(self, df):\n",
    "        \"\"\"Create comprehensive customer segments including engagement\"\"\"\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            print(f\"Creating segments for {len(df):,} customers\")\n",
    "            segment_components = []\n",
    "\n",
    "            # 1. Create RFM Segments\n",
    "            if self.segment_config['use_rfm']:\n",
    "                df = self._create_rfm_segments(df)\n",
    "                segment_components.append('rfm_segment')\n",
    "                print(f\"\\nCreated {df['rfm_segment'].nunique():,} RFM segments\")\n",
    "\n",
    "            # 2. Create Engagement Segments\n",
    "            if self.segment_config.get('use_engagement', True):\n",
    "                df = self._create_engagement_segments(df)\n",
    "                segment_components.append('engagement_level')\n",
    "                print(f\"\\nCreated {df['engagement_level'].nunique():,} engagement segments\")\n",
    "\n",
    "            # 3. Create Loyalty Segments\n",
    "            if self.segment_config.get('use_loyalty', True):\n",
    "                df = self._create_loyalty_segments(df)\n",
    "                segment_components.append('loyalty_segment')\n",
    "                print(f\"\\nCreated {df['loyalty_segment'].nunique():,} loyalty segments\")\n",
    "\n",
    "            # 4. Create Channel Segments\n",
    "            if self.segment_config.get('use_channel', True):\n",
    "                df = self._determine_channel(df)\n",
    "                segment_components.append('channel_segment')\n",
    "                print(f\"\\nCreated {df['channel_segment'].nunique():,} channel segments\")\n",
    "\n",
    "            # 5. Create Combined Segments\n",
    "            use_combined = self.segment_config.get('use_combined', True)\n",
    "            if use_combined and len(segment_components) > 1:\n",
    "                df['customer_segment'] = df[segment_components].apply(\n",
    "                    lambda x: '_'.join(x.astype(str)), axis=1\n",
    "                )\n",
    "\n",
    "                # Handle small segments if enabled\n",
    "                if self.segment_config.get('merge_small_segments', True):\n",
    "                    segment_counts = df['customer_segment'].value_counts()\n",
    "                    small_segments = segment_counts[\n",
    "                        segment_counts < self.segment_config['min_segment_size']\n",
    "                    ].index\n",
    "                    if len(small_segments) > 0:\n",
    "                        df.loc[df['customer_segment'].isin(small_segments), 'customer_segment'] = 'other'\n",
    "\n",
    "            # 6. Create Cohort Groups\n",
    "            if self.segment_config.get('use_cohorts', True):\n",
    "                df = self._create_cohort_groups(df)\n",
    "                print(f\"\\nCreated {df['cohort_segment'].nunique():,} cohort groups\")\n",
    "\n",
    "            # 7. Create final group indices\n",
    "            grouping_column = 'cohort_segment' if self.segment_config.get('use_cohorts', True) else 'customer_segment'\n",
    "            unique_groups = df[grouping_column].unique()\n",
    "            group_mapping = {group: idx for idx, group in enumerate(unique_groups)}\n",
    "            df['group_idx'] = df[grouping_column].map(group_mapping)\n",
    "\n",
    "            # Store grouping info\n",
    "            self.groups = unique_groups\n",
    "            self.coords = {\n",
    "                \"group_idx\": np.arange(len(unique_groups)),\n",
    "                \"group\": unique_groups\n",
    "            }\n",
    "\n",
    "            print(f\"\\nCreated {len(unique_groups):,} total groups with indices\")\n",
    "\n",
    "            return df  # Return the segmented DataFrame\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Segmentation error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_rfm_segments(self, df):\n",
    "        \"\"\"Create RFM segments with configurable bins\"\"\"\n",
    "        try:\n",
    "            self.segment_bins['rfm'] = {}\n",
    "\n",
    "            # Create segments for each RFM component\n",
    "            for metric in ['frequency', 'recency', 'monetary']:\n",
    "                n_bins = self.segment_config['rfm_bins'][metric]\n",
    "\n",
    "                # Get the data range for the current metric\n",
    "                metric_min = df[metric].min()\n",
    "                metric_max = df[metric].max()\n",
    "\n",
    "                # Calculate the bin edges\n",
    "                bin_edges = np.linspace(metric_min, metric_max, n_bins + 1)\n",
    "\n",
    "                # Create labels based on the number of bins\n",
    "                labels = [f\"{metric.capitalize()} {i+1}\" for i in range(n_bins)]\n",
    "\n",
    "                # Create segments using pd.cut()\n",
    "                segment_col = f\"{metric[0].lower()}_segment\"\n",
    "                df[segment_col] = pd.cut(\n",
    "                    df[metric],\n",
    "                    bins=bin_edges,\n",
    "                    labels=labels,\n",
    "                    include_lowest=True\n",
    "                )\n",
    "\n",
    "                print(f\"\\n{segment_col} distribution:\")\n",
    "                print(df[segment_col].value_counts().sort_index())\n",
    "\n",
    "            # Combine RFM segments\n",
    "            df['rfm_segment'] = (\n",
    "                df['r_segment'].astype(str) + '_' +\n",
    "                df['f_segment'].astype(str) + '_' +\n",
    "                df['m_segment'].astype(str)\n",
    "            )\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"RFM segmentation error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_engagement_segments(self, df):\n",
    "        \"\"\"Create engagement segments based on customer behavior\"\"\"\n",
    "        try:\n",
    "            # 1. Binary engagement metrics\n",
    "            engagement_score = 0\n",
    "            metrics = self.segment_config['engagement_metrics']\n",
    "\n",
    "            for metric in metrics:\n",
    "                if metric in df.columns:\n",
    "                    engagement_score += df[metric]\n",
    "\n",
    "            # Create engagement level with proper bin handling\n",
    "            try:\n",
    "                print(\"\\nEngagement score summary:\")\n",
    "                print(engagement_score.describe())\n",
    "\n",
    "                # Create explicit bins based on the data\n",
    "                n_bins = self.segment_config['engagement_bins']\n",
    "                labels = ['low', 'high'] if n_bins == 2 else ['low', 'medium', 'high']\n",
    "\n",
    "                df['engagement_level'] = pd.qcut(\n",
    "                    engagement_score,\n",
    "                    q=n_bins,\n",
    "                    labels=labels,\n",
    "                    duplicates='drop'\n",
    "                )\n",
    "\n",
    "                print(\"\\nEngagement level distribution:\")\n",
    "                print(df['engagement_level'].value_counts().sort_index())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating engagement levels: {str(e)}\")\n",
    "                # Fallback to binary segmentation\n",
    "                median = engagement_score.median()\n",
    "                df['engagement_level'] = np.where(\n",
    "                    engagement_score > median,\n",
    "                    'high', 'low'\n",
    "                )\n",
    "                print(\"Fell back to binary engagement segmentation\")\n",
    "\n",
    "            # Handle small segments if enabled\n",
    "            if self.segment_config['merge_small_segments']:\n",
    "                segment_counts = df['engagement_level'].value_counts()\n",
    "                small_segments = segment_counts[\n",
    "                    segment_counts < self.segment_config['min_segment_size']\n",
    "                ].index\n",
    "                if len(small_segments) > 0:\n",
    "                    df.loc[df['engagement_level'].isin(small_segments), 'engagement_level'] = 'other'\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Engagement segmentation error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_loyalty_segments(self, df):\n",
    "        \"\"\"Create loyalty segments with configurable bins\"\"\"\n",
    "        try:\n",
    "            if 'loyalty_points' in df.columns:\n",
    "                try:\n",
    "                    n_bins = self.segment_config['loyalty_bins']\n",
    "                    labels = ['low', 'high'] if n_bins == 2 else ['low', 'medium', 'high']\n",
    "\n",
    "                    # Calculate loyalty points bins\n",
    "                    loyalty_stats = df['loyalty_points'].describe()\n",
    "                    print(\"\\nLoyalty points summary:\")\n",
    "                    print(loyalty_stats)\n",
    "\n",
    "                    # Create segments using qcut for even distribution\n",
    "                    df['loyalty_segment'] = pd.qcut(\n",
    "                        df['loyalty_points'],\n",
    "                        q=n_bins,\n",
    "                        labels=labels,\n",
    "                        duplicates='drop'\n",
    "                    )\n",
    "\n",
    "                    print(\"\\nLoyalty segment distribution:\")\n",
    "                    print(df['loyalty_segment'].value_counts().sort_index())\n",
    "\n",
    "                    # Handle small segments if enabled\n",
    "                    if self.segment_config['merge_small_segments']:\n",
    "                        segment_counts = df['loyalty_segment'].value_counts()\n",
    "                        small_segments = segment_counts[\n",
    "                            segment_counts < self.segment_config['min_segment_size']\n",
    "                        ].index\n",
    "                        if len(small_segments) > 0:\n",
    "                            df.loc[df['loyalty_segment'].isin(small_segments), 'loyalty_segment'] = 'other'\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating loyalty segments: {str(e)}\")\n",
    "                    # Fallback to binary segmentation\n",
    "                    median = df['loyalty_points'].median()\n",
    "                    df['loyalty_segment'] = np.where(\n",
    "                        df['loyalty_points'] > median,\n",
    "                        'high', 'low'\n",
    "                    )\n",
    "                    print(\"Fell back to binary loyalty segmentation\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Loyalty segmentation error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_cohort_groups(self, df):\n",
    "        \"\"\"Create cohort groups for hierarchical modeling\"\"\"\n",
    "        try:\n",
    "            # Extract cohort features\n",
    "            df['cohort_year'] = df['cohort_month'].dt.year\n",
    "\n",
    "            if self.segment_config['cohort_type'] == 'quarterly':\n",
    "                df['cohort_period'] = df['cohort_month'].dt.quarter\n",
    "                period_name = 'quarter'\n",
    "            else:  # monthly\n",
    "                df['cohort_period'] = df['cohort_month'].dt.month\n",
    "                period_name = 'month'\n",
    "\n",
    "            # Create cohort-segment combination\n",
    "            if 'customer_segment' in df.columns:\n",
    "                df['cohort_segment'] = (\n",
    "                    df['cohort_year'].astype(str) + '_' +\n",
    "                    df['cohort_period'].astype(str) + '_' +\n",
    "                    df['customer_segment']\n",
    "                )\n",
    "            else:\n",
    "                df['cohort_segment'] = (\n",
    "                    df['cohort_year'].astype(str) + '_' +\n",
    "                    df['cohort_period'].astype(str)\n",
    "                )\n",
    "\n",
    "            # Handle small cohorts if enabled\n",
    "            if self.segment_config['merge_small_segments']:\n",
    "                segment_counts = df['cohort_segment'].value_counts()\n",
    "                small_segments = segment_counts[\n",
    "                    segment_counts < self.segment_config['min_segment_size']\n",
    "                ].index\n",
    "\n",
    "                if len(small_segments) > 0:\n",
    "                    # For cohorts, merge into nearest time period instead of 'other'\n",
    "                    for small_seg in small_segments:\n",
    "                        year, period, *rest = small_seg.split('_')\n",
    "                        # Find closest cohort in time\n",
    "                        nearby_cohorts = [\n",
    "                            seg for seg in segment_counts.index\n",
    "                            if seg not in small_segments and\n",
    "                            seg.startswith(f\"{year}_\")\n",
    "                        ]\n",
    "                        if nearby_cohorts:\n",
    "                            closest_cohort = min(nearby_cohorts, key=lambda x: abs(\n",
    "                                int(x.split('_')[1]) - int(period)\n",
    "                            ))\n",
    "                            df.loc[df['cohort_segment'] == small_seg, 'cohort_segment'] = closest_cohort\n",
    "\n",
    "            print(f\"\\nCreated {df['cohort_segment'].nunique():,} cohort groups\")\n",
    "            print(f\"\\nCohort distribution by {period_name}:\")\n",
    "            print(df.groupby(['cohort_year', 'cohort_period']).size().unstack())\n",
    "\n",
    "            # Store cohort information\n",
    "            self.cohort_info = {\n",
    "                'type': self.segment_config['cohort_type'],\n",
    "                'years': sorted(df['cohort_year'].unique()),\n",
    "                'periods': sorted(df['cohort_period'].unique())\n",
    "            }\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Cohort grouping error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _determine_channel(self, df):\n",
    "        \"\"\"Determine customer channel preference\"\"\"\n",
    "        df['has_online_purchases'] = df['has_online_purchases'].astype(bool)\n",
    "        df['has_store_purchases'] = df['has_store_purchases'].astype(bool)\n",
    "\n",
    "        df['channel_segment'] = df.apply(\n",
    "            lambda row: self._determine_channel(row['has_online_purchases'], row['has_store_purchases']),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _determine_channel(self, online, store):\n",
    "        \"\"\"Determine customer channel preference\"\"\"\n",
    "        if online and store:\n",
    "            return 'Multichannel'\n",
    "        elif online:\n",
    "            return 'Online'\n",
    "        elif store:\n",
    "            return 'Store'\n",
    "        return 'Unknown'\n",
    "\n",
    "    def process_batches(self, df):\n",
    "        \"\"\"Process data in batches with proper type handling\"\"\"\n",
    "        try:\n",
    "            # First ensure correct dtypes\n",
    "            numeric_cols = [\n",
    "                'recency', 'frequency', 'monetary',\n",
    "                'total_revenue', 'revenue_trend', 'avg_transaction_value',\n",
    "                'customer_age_days', 'distinct_categories', 'distinct_brands',\n",
    "                'avg_interpurchase_days', 'total_discount_amount',\n",
    "                'avg_discount_amount', 'discount_rate'\n",
    "            ]\n",
    "\n",
    "            bool_cols = [\n",
    "                'has_online_purchases', 'has_store_purchases',\n",
    "                'sms_active', 'email_active', 'is_loyalty_member'\n",
    "            ]\n",
    "\n",
    "            int_cols = ['frequency', 'loyalty_points', 'group_idx']\n",
    "\n",
    "            # Convert dtypes\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                    df[col] = df[col].astype('float32')\n",
    "\n",
    "            for col in bool_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype('int32')\n",
    "\n",
    "            for col in int_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype('int32')\n",
    "\n",
    "            # Process in batches\n",
    "            batch_size = self.config['batch_size']\n",
    "            n_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "\n",
    "            print(f\"\\nProcessing {len(df):,} records in {n_batches:,} batches\")\n",
    "\n",
    "            processed_batches = []\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(df))\n",
    "\n",
    "                print(f\"Processing batch {i+1}/{n_batches}\")\n",
    "                batch = df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "                # Process batch\n",
    "                processed_batch = self.process_batch(batch)\n",
    "                if processed_batch is not None:\n",
    "                    processed_batches.append(processed_batch)\n",
    "\n",
    "                # Clear batch from memory\n",
    "                del batch\n",
    "                gc.collect()\n",
    "\n",
    "            # Concatenate processed batches\n",
    "            result_df = pd.concat(processed_batches, ignore_index=True)\n",
    "\n",
    "            # Clear processed batches from memory\n",
    "            del processed_batches\n",
    "            gc.collect()\n",
    "\n",
    "            # Final type check\n",
    "            print(\"\\nChecking final data types...\")\n",
    "            for col in result_df.columns:\n",
    "                print(f\"{col}: {result_df[col].dtype}\")\n",
    "\n",
    "            return result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Batch processing error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"Process a single batch with outlier handling and cleaning\"\"\"\n",
    "        try:\n",
    "            # Basic cleaning\n",
    "            batch = batch[batch['monetary'] > 0]\n",
    "\n",
    "            # Handle outliers\n",
    "            for col in ['monetary', 'frequency', 'avg_transaction_value']:\n",
    "                if col in batch.columns:\n",
    "                    Q1 = batch[col].quantile(0.25)\n",
    "                    Q3 = batch[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "\n",
    "                    lower_bound = Q1 - 3 * IQR\n",
    "                    upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "                    # Cap values instead of removing\n",
    "                    batch[col] = batch[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "            # Additional cleaning steps\n",
    "            if 'avg_interpurchase_days' in batch.columns:\n",
    "                batch['avg_interpurchase_days'] = batch['avg_interpurchase_days'].clip(lower=0)\n",
    "\n",
    "            if 'recency' in batch.columns:\n",
    "                batch['recency'] = batch['recency'].clip(lower=0)\n",
    "\n",
    "            if 'customer_age_days' in batch.columns:\n",
    "                batch['customer_age_days'] = batch['customer_age_days'].clip(lower=1)\n",
    "\n",
    "            # Ensure proper relationships\n",
    "            batch = batch[\n",
    "                (batch['customer_age_days'] >= batch['recency']) &\n",
    "                (batch['monetary'] >= 0) &\n",
    "                (batch['frequency'] >= 1)\n",
    "            ]\n",
    "\n",
    "            return batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Batch processing error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # 4. Model Building Methods\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build hierarchical BG/NBD model with covariates\"\"\"\n",
    "        try:\n",
    "            if self.data is None:\n",
    "                raise ValueError(\"No data available. Run data processing first.\")\n",
    "\n",
    "            print(\"\\nBuilding model...\")\n",
    "            print(\"Preparing model data...\")\n",
    "\n",
    "            # 1. Prepare model data\n",
    "            self.model_data = self._prepare_model_data()\n",
    "\n",
    "            # 2. Build PyMC model\n",
    "            print(\"Building PyMC model...\")\n",
    "            with pm.Model() as self.model:\n",
    "                # Add priors\n",
    "                self._add_hierarchical_priors()\n",
    "\n",
    "                # Add covariate effects\n",
    "                self._add_covariate_effects()\n",
    "\n",
    "                # Add likelihood\n",
    "                x_tensor = pt.as_tensor_variable(self.model_data['frequency'])\n",
    "                t_x_tensor = pt.as_tensor_variable(self.model_data['recency'])\n",
    "                T_tensor = pt.as_tensor_variable(self.model_data['T'])\n",
    "                x_zero_tensor = pt.as_tensor_variable(self.model_data['frequency'] == 0)\n",
    "\n",
    "                # Store log likelihood for use during training\n",
    "                self.model.log_likelihood = self.compute_log_likelihood(\n",
    "                    x_tensor, t_x_tensor, T_tensor, x_zero_tensor)\n",
    "\n",
    "            print(\"Model built successfully\")\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model building error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _prepare_model_data(self):\n",
    "        \"\"\"Prepare data for PyMC model with covariates\"\"\"\n",
    "        try:\n",
    "            if self.data is None:\n",
    "                raise ValueError(\"No data available. Run data processing first.\")\n",
    "\n",
    "            if 'group_idx' not in self.data.columns:\n",
    "                raise ValueError(\"group_idx column missing. Run create_segments first.\")\n",
    "\n",
    "            # Convert main variables to float32\n",
    "            model_data = {\n",
    "                'frequency': self.data['frequency'].values.astype('float32'),\n",
    "                'recency': self.data['recency'].values.astype('float32'),\n",
    "                'T': self.data['customer_age_days'].values.astype('float32'),\n",
    "                'monetary': self.data['monetary'].values.astype('float32'),\n",
    "                'avg_transaction': self.data['avg_transaction_value'].values.astype('float32'),\n",
    "                'group_idx': self.data['group_idx'].values.astype('int32')\n",
    "            }\n",
    "\n",
    "            # Prepare engagement covariates\n",
    "            engagement_cols = [\n",
    "                'sms_active', 'email_active', 'is_loyalty_member',\n",
    "                'loyalty_points'\n",
    "            ]\n",
    "\n",
    "            # Standardize numeric covariates\n",
    "            numeric_covs = ['loyalty_points', 'avg_interpurchase_days',\n",
    "                          'distinct_categories', 'distinct_brands']\n",
    "\n",
    "            self.scalers = {}  # Store scalers for later use/inverse transform\n",
    "            for col in numeric_covs:\n",
    "                if col in self.data.columns:\n",
    "                    scaler = StandardScaler()\n",
    "                    # Reshape to 2D array for sklearn (-1 means infer length)\n",
    "                    reshaped_data = self.data[col].values.reshape(-1, 1)\n",
    "                    # Fit scaler and transform data\n",
    "                    # This standardizes the data to mean=0, std=1\n",
    "                    scaled_data = scaler.fit_transform(reshaped_data)\n",
    "                    # Convert to float32 and flatten back to 1D\n",
    "                    model_data[f'{col}_scaled'] = scaled_data.astype('float32').flatten()\n",
    "                    # Store scaler for this column\n",
    "                    self.scalers[col] = scaler\n",
    "\n",
    "                    if self.config.get('VERBOSE', False):\n",
    "                        print(f\"\\nScaling {col}:\")\n",
    "                        print(f\"Mean: {scaler.mean_[0]:.2f}\")\n",
    "                        print(f\"Std: {scaler.scale_[0]:.2f}\")\n",
    "\n",
    "            # Binary covariates\n",
    "            binary_covs = ['has_online_purchases', 'has_store_purchases',\n",
    "                          'sms_active', 'email_active', 'is_loyalty_member']\n",
    "\n",
    "            for col in binary_covs:\n",
    "                if col in self.data.columns:\n",
    "                    model_data[col] = self.data[col].values.astype('float32')\n",
    "\n",
    "            print(\"\\nPrepared model data:\")\n",
    "            for key, value in model_data.items():\n",
    "                print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "\n",
    "            return model_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing model data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _add_hierarchical_priors(self):\n",
    "        \"\"\"Add hierarchical priors to PyMC model\"\"\"\n",
    "        try:\n",
    "            n_groups = len(self.groups)\n",
    "\n",
    "            # Group-level hyperpriors\n",
    "            self.hyper_priors = {\n",
    "                'r': {\n",
    "                    'alpha': pm.Gamma('r_alpha', alpha=2, beta=1),\n",
    "                    'beta': pm.Gamma('r_beta', alpha=2, beta=1)\n",
    "                },\n",
    "                'alpha': {\n",
    "                    'mu': pm.Gamma('alpha_mu', alpha=2, beta=1),\n",
    "                    'sigma': pm.HalfNormal('alpha_sigma', sigma=1)\n",
    "                },\n",
    "                's': {\n",
    "                    'alpha': pm.Gamma('s_alpha', alpha=2, beta=1),\n",
    "                    'beta': pm.Gamma('s_beta', alpha=2, beta=1)\n",
    "                },\n",
    "                'beta': {\n",
    "                    'mu': pm.Gamma('beta_mu', alpha=2, beta=1),\n",
    "                    'sigma': pm.HalfNormal('beta_sigma', sigma=1)\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Group-level parameters\n",
    "            self.group_params = {\n",
    "                'r': pm.Gamma('r',\n",
    "                    alpha=self.hyper_priors['r']['alpha'],\n",
    "                    beta=self.hyper_priors['r']['beta'],\n",
    "                    shape=n_groups\n",
    "                ),\n",
    "                'alpha': pm.Gamma('alpha',\n",
    "                    mu=self.hyper_priors['alpha']['mu'],\n",
    "                    sigma=self.hyper_priors['alpha']['sigma'],\n",
    "                    shape=n_groups\n",
    "                ),\n",
    "                's': pm.Gamma('s',\n",
    "                    alpha=self.hyper_priors['s']['alpha'],\n",
    "                    beta=self.hyper_priors['s']['beta'],\n",
    "                    shape=n_groups\n",
    "                ),\n",
    "                'beta': pm.Gamma('beta',\n",
    "                    mu=self.hyper_priors['beta']['mu'],\n",
    "                    sigma=self.hyper_priors['beta']['sigma'],\n",
    "                    shape=n_groups\n",
    "                )\n",
    "            }\n",
    "\n",
    "            print(\"\\nAdded hierarchical priors\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding hierarchical priors: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _add_covariate_effects(self):\n",
    "        \"\"\"Add covariate effects to the model\"\"\"\n",
    "        try:\n",
    "            # Covariate coefficients for each parameter\n",
    "            self.coef_priors = {}\n",
    "\n",
    "            # Define covariate groups\n",
    "            covariate_groups = {\n",
    "                'transaction': ['avg_interpurchase_days_scaled'],\n",
    "                'customer': ['distinct_categories_scaled', 'distinct_brands_scaled'],\n",
    "                'channel': ['has_online_purchases', 'has_store_purchases'],\n",
    "                'engagement': ['sms_active', 'email_active', 'is_loyalty_member',\n",
    "                              'loyalty_points_scaled']\n",
    "            }\n",
    "\n",
    "            # Add coefficient priors for each parameter and covariate group\n",
    "            for param in ['r', 'alpha']:\n",
    "                self.coef_priors[param] = {}\n",
    "\n",
    "                for group, covariates in covariate_groups.items():\n",
    "                    for cov in covariates:\n",
    "                        if cov in self.model_data:\n",
    "                            coef_name = f\"{param}_{cov}_coef\"\n",
    "                            self.coef_priors[param][cov] = pm.Normal(\n",
    "                                coef_name,\n",
    "                                mu=0,\n",
    "                                sigma=1\n",
    "                            )\n",
    "\n",
    "            print(\"\\nAdded covariate effects\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding covariate effects: {str(e)}\")\n",
    "            raise\n",
    "    def compute_log_likelihood(self, x, t_x, T, x_zero):\n",
    "        \"\"\"Compute log-likelihood for the hierarchical BG/NBD model\"\"\"\n",
    "        try:\n",
    "            import pytensor.tensor as pt\n",
    "\n",
    "            # Convert inputs to tensors\n",
    "            x = pt.as_tensor_variable(x)\n",
    "            t_x = pt.as_tensor_variable(t_x)\n",
    "            T = pt.as_tensor_variable(T)\n",
    "            x_zero = pt.as_tensor_variable(x_zero)\n",
    "\n",
    "            # Get group indices\n",
    "            group_idx = self.model_data['group_idx']\n",
    "\n",
    "            # Create distributions\n",
    "            purchase_dist = pm.NegativeBinomial.dist(\n",
    "                mu=pt.exp(self.group_params['r'][group_idx]),\n",
    "                alpha=pt.exp(self.group_params['alpha'][group_idx])\n",
    "            )\n",
    "            dropout_dist = pm.Gamma.dist(\n",
    "                alpha=pt.exp(self.group_params['s'][group_idx]),\n",
    "                beta=pt.exp(self.group_params['beta'][group_idx])\n",
    "            )\n",
    "\n",
    "            # Calculate likelihoods using correct pm.logp syntax\n",
    "            purchase_likelihood = pm.logp(rv=purchase_dist, value=x)\n",
    "            dropout_likelihood = pm.logp(rv=dropout_dist, value=T - t_x)\n",
    "\n",
    "            # Calculate alive probability\n",
    "            p_alive = pt.exp(-pt.exp(self.group_params['alpha'][group_idx]) * T)\n",
    "\n",
    "            # Combine components\n",
    "            log_likelihood = (\n",
    "                purchase_likelihood +\n",
    "                dropout_likelihood * (1 - x_zero) +\n",
    "                pt.log(p_alive + (1 - p_alive) * x_zero)\n",
    "            )\n",
    "\n",
    "            return log_likelihood\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing log-likelihood: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_covariate_effects(self, param, group_idx):\n",
    "        \"\"\"Calculate combined covariate effects for a parameter\"\"\"\n",
    "        try:\n",
    "            # Start with group-level parameter\n",
    "            effects = pt.log(self.group_params[param][group_idx])\n",
    "\n",
    "            # Add covariate effects\n",
    "            for cov, coef in self.coef_priors[param].items():\n",
    "                effects = effects + coef * self.model_data[cov]\n",
    "\n",
    "            return effects\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating covariate effects: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # 5. Training and Convergence Methods\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train model with GPU memory optimization and log likelihood computation\"\"\"\n",
    "        try:\n",
    "            if self.model is None:\n",
    "                raise ValueError(\"No model built. Call build_model first.\")\n",
    "\n",
    "            # Optimize GPU memory before training\n",
    "            if self.gpu_enabled:\n",
    "                self.optimize_gpu_memory()\n",
    "\n",
    "            print(\"\\nTraining model...\")\n",
    "            with self.model:\n",
    "                # Setup NUTS sampler with optimized parameters\n",
    "                step = pm.NUTS(\n",
    "                    target_accept=self.config['target_accept'],\n",
    "                    max_treedepth=self.config['max_treedepth']\n",
    "                )\n",
    "\n",
    "                # Use gradient accumulation if enabled\n",
    "                if self.config.get('gradient_accumulation', 0) > 1:\n",
    "                    draws_per_step = self.config['mcmc_samples'] // self.config['gradient_accumulation']\n",
    "                else:\n",
    "                    draws_per_step = self.config['mcmc_samples']\n",
    "\n",
    "                # Run sampling with memory monitoring\n",
    "                try:\n",
    "                    for i in range(self.config.get('gradient_accumulation', 1)):\n",
    "                        if i > 0:\n",
    "                            print(f\"\\nGradient accumulation step {i+1}\")\n",
    "\n",
    "                        # Monitor GPU memory before sampling\n",
    "                        if self.gpu_enabled:\n",
    "                            self._monitor_gpu()\n",
    "\n",
    "                        self.trace = pm.sample(\n",
    "                            draws=draws_per_step,\n",
    "                            tune=self.config['mcmc_tune'],\n",
    "                            chains=self.config['chains'],\n",
    "                            cores=self.config['cores'],\n",
    "                            random_seed=self.config['random_seed'],\n",
    "                            step=step,\n",
    "                            return_inferencedata=True,\n",
    "                            progressbar=True,\n",
    "                            compute_convergence_checks=True,\n",
    "                            discard_tuned_samples=False,\n",
    "                            mp_ctx='spawn',  # Keep stable multiprocessing\n",
    "                            idata_kwargs={\"log_likelihood\": True}  # Enable log likelihood computation\n",
    "                        )\n",
    "\n",
    "                        # If log-likelihood not computed during sampling, compute it explicitly\n",
    "                        if not hasattr(self.trace, 'log_likelihood'):\n",
    "                            print(\"\\nComputing log-likelihood...\")\n",
    "                            try:\n",
    "                                with self.model:\n",
    "                                    log_like = pm.compute_log_likelihood(\n",
    "                                        self.trace,\n",
    "                                        var_names=['frequency', 'recency', 'T']\n",
    "                                    )\n",
    "                                    self.trace.add_groups({'log_likelihood': log_like})\n",
    "                            except Exception as ll_error:\n",
    "                                print(f\"Warning: Could not compute log-likelihood: {str(ll_error)}\")\n",
    "                                # Fallback to manual computation\n",
    "                                log_like = self.compute_log_likelihood(\n",
    "                                    self.model_data['frequency'],\n",
    "                                    self.model_data['recency'],\n",
    "                                    self.model_data['T'],\n",
    "                                    self.model_data['frequency'] == 0\n",
    "                                )\n",
    "                                self.trace.add_groups({'log_likelihood': log_like})\n",
    "\n",
    "                        # Check convergence metrics\n",
    "                        converged = self._check_convergence()\n",
    "\n",
    "                        # Clear memory between accumulation steps\n",
    "                        if i < self.config.get('gradient_accumulation', 1) - 1:\n",
    "                            self._clear_memory()\n",
    "                            if self.gpu_enabled:\n",
    "                                self._monitor_gpu()\n",
    "\n",
    "                        # Early stopping if convergence is poor\n",
    "                        if not converged:\n",
    "                            print(\"\\nWarning: Poor convergence detected\")\n",
    "                            if i > 0:  # Only warn if not first iteration\n",
    "                                print(\"Consider reducing model complexity or increasing tuning steps\")\n",
    "\n",
    "                        # Monitor resources\n",
    "                        if self.config.get('monitor_resources', True):\n",
    "                            self._monitor_resources()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nSampling error: {str(e)}\")\n",
    "                    if hasattr(self, 'trace') and self.trace is not None:\n",
    "                        print(\"\\nPartial trace available - attempting to salvage results\")\n",
    "                        self._check_convergence()  # Check what we have\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "                finally:\n",
    "                    # Final convergence check\n",
    "                    if hasattr(self, 'trace') and self.trace is not None:\n",
    "                        print(\"\\nFinal convergence check:\")\n",
    "                        self._check_convergence()\n",
    "\n",
    "                    # Clear memory\n",
    "                    self._clear_memory()\n",
    "\n",
    "                return self\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nTraining error: {str(e)}\")\n",
    "            return self  # Return self even on error to allow saving partial results\n",
    "\n",
    "    def optimize_gpu_memory(self):\n",
    "        \"\"\"Optimize GPU memory usage\"\"\"\n",
    "        try:\n",
    "            import torch\n",
    "            import gc\n",
    "\n",
    "            if not self.gpu_enabled:\n",
    "                return False\n",
    "\n",
    "            # 1. Memory Management Functions\n",
    "            def clear_gpu_memory():\n",
    "                \"\"\"Clear unused memory\"\"\"\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'memory_summary'):\n",
    "                    print(\"\\nGPU Memory Summary:\")\n",
    "                    print(torch.cuda.memory_summary())\n",
    "\n",
    "            def get_gpu_memory_usage():\n",
    "                \"\"\"Get current memory usage\"\"\"\n",
    "                allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "                reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "                return allocated, reserved\n",
    "\n",
    "            # 2. Set GPU Memory Optimization Settings\n",
    "            torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes\n",
    "            if hasattr(torch, 'backends') and hasattr(torch.backends, 'cuda') and \\\n",
    "              hasattr(torch.backends.cuda, 'matmul') and \\\n",
    "              hasattr(torch.backends.cuda.matmul, 'allow_tf32'):\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True  # Use TF32 for better performance\n",
    "\n",
    "            # 3. Implement Memory-Efficient Settings in Config\n",
    "            self.config.update({\n",
    "                'gradient_accumulation': 4,  # Reduce memory by accumulating gradients\n",
    "                'mixed_precision': True,     # Use mixed precision training\n",
    "                'memory_efficient': True     # Enable memory-efficient options\n",
    "            })\n",
    "\n",
    "            # 4. Print Initial Memory State\n",
    "            allocated, reserved = get_gpu_memory_usage()\n",
    "            print(\"\\nInitial GPU Memory State:\")\n",
    "            print(f\"Allocated: {allocated:.1f} MB\")\n",
    "            print(f\"Reserved:  {reserved:.1f} MB\")\n",
    "\n",
    "            # 5. Clear Memory\n",
    "            clear_gpu_memory()\n",
    "\n",
    "            # 6. Optimize Batch Size Based on GPU Memory\n",
    "            try:\n",
    "                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "                available_memory = total_memory * 0.85  # Use 85% of total memory\n",
    "\n",
    "                # Estimate memory per sample (adjust these based on your model)\n",
    "                estimated_memory_per_sample = 0.5  # MB per sample\n",
    "                optimal_batch_size = int(available_memory / estimated_memory_per_sample)\n",
    "\n",
    "                # Cap batch size at reasonable limits\n",
    "                optimal_batch_size = min(max(1000, optimal_batch_size), 10000)\n",
    "\n",
    "                self.config['batch_size'] = optimal_batch_size\n",
    "                print(f\"\\nOptimized batch size: {optimal_batch_size}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error optimizing batch size: {str(e)}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"GPU memory optimization error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def optimize_resources(self):\n",
    "        \"\"\"Automatically optimize resources based on available memory\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            import torch\n",
    "            import gc\n",
    "\n",
    "            # 1. System Memory Analysis\n",
    "            system_memory = psutil.virtual_memory()\n",
    "            total_ram_gb = system_memory.total / (1024**3)\n",
    "            available_ram_gb = system_memory.available / (1024**3)\n",
    "\n",
    "            print(\"\\nSystem Memory Analysis:\")\n",
    "            print(f\"Total RAM: {total_ram_gb:.1f} GB\")\n",
    "            print(f\"Available RAM: {available_ram_gb:.1f} GB\")\n",
    "\n",
    "            # 2. GPU Memory Analysis\n",
    "            if self.gpu_enabled:\n",
    "                gpu_memory = {}\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                    reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "                    allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "                    available = total - allocated\n",
    "\n",
    "                    gpu_memory[i] = {\n",
    "                        'total': total,\n",
    "                        'available': available,\n",
    "                        'reserved': reserved,\n",
    "                        'allocated': allocated\n",
    "                    }\n",
    "\n",
    "                    print(f\"\\nGPU {i} Memory:\")\n",
    "                    print(f\"Total: {total:.1f} GB\")\n",
    "                    print(f\"Available: {available:.1f} GB\")\n",
    "\n",
    "            # 3. Calculate Optimal Parameters\n",
    "            def calculate_optimal_batch_size():\n",
    "                \"\"\"Calculate optimal batch size based on available memory\"\"\"\n",
    "                if self.gpu_enabled:\n",
    "                    # Use minimum available GPU memory across all GPUs\n",
    "                    min_available_gpu = min(gpu['available'] for gpu in gpu_memory.values())\n",
    "                    memory_for_batches = min_available_gpu * 0.8  # Use 80% of available GPU memory\n",
    "                else:\n",
    "                    memory_for_batches = available_ram_gb * 0.8\n",
    "\n",
    "                # Estimate memory per sample based on data size\n",
    "                sample_size_mb = (\n",
    "                    self.data.memory_usage().sum() / len(self.data) / 1024**2\n",
    "                ) if self.data is not None else 0.5\n",
    "\n",
    "                optimal_batch_size = int(\n",
    "                    (memory_for_batches * 1024) / sample_size_mb\n",
    "                )\n",
    "\n",
    "                # Apply reasonable bounds\n",
    "                return min(max(1000, optimal_batch_size), 10000)\n",
    "\n",
    "            def calculate_optimal_chains():\n",
    "                \"\"\"Calculate optimal number of chains based on CPU cores and memory\"\"\"\n",
    "                cpu_count = psutil.cpu_count(logical=False)\n",
    "                memory_based_chains = int(available_ram_gb / 20)  # Assume 20GB per chain\n",
    "\n",
    "                if self.gpu_enabled:\n",
    "                    gpu_based_chains = sum(\n",
    "                        int(mem['available'] / 20) for mem in gpu_memory.values()\n",
    "                    )\n",
    "                    return min(cpu_count, memory_based_chains, gpu_based_chains, 4)\n",
    "\n",
    "                return min(cpu_count, memory_based_chains, 4)\n",
    "\n",
    "            # 4. Update Configuration\n",
    "            optimal_batch_size = calculate_optimal_batch_size()\n",
    "            optimal_chains = calculate_optimal_chains()\n",
    "\n",
    "            memory_optimized_config = {\n",
    "                'batch_size': optimal_batch_size,\n",
    "                'chains': optimal_chains,\n",
    "                'cores': optimal_chains,  # Match cores to chains\n",
    "                'gradient_accumulation': max(1, int(4096 / optimal_batch_size)),\n",
    "                'mixed_precision': True if self.gpu_enabled else False,\n",
    "\n",
    "                # Memory thresholds\n",
    "                'memory_warn_threshold': 0.85,\n",
    "                'memory_critical_threshold': 0.95,\n",
    "\n",
    "                # Automatic cleanup triggers\n",
    "                'auto_cleanup_threshold': 0.9,\n",
    "                'cleanup_interval': 100  # Clean every N batches\n",
    "            }\n",
    "\n",
    "            # 5. Update instance configuration\n",
    "            self.config.update(memory_optimized_config)\n",
    "\n",
    "            # 6. Print Optimization Summary\n",
    "            print(\"\\nResource Optimization Summary:\")\n",
    "            print(f\"Optimal Batch Size: {optimal_batch_size}\")\n",
    "            print(f\"Optimal Chains: {optimal_chains}\")\n",
    "            print(f\"Gradient Accumulation Steps: {memory_optimized_config['gradient_accumulation']}\")\n",
    "\n",
    "            # 7. Setup Memory Monitoring\n",
    "            def setup_memory_monitoring():\n",
    "                \"\"\"Setup automatic memory monitoring and cleanup\"\"\"\n",
    "                def memory_monitor():\n",
    "                    while True:\n",
    "                        current_usage = (\n",
    "                            torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n",
    "                            if self.gpu_enabled else\n",
    "                            psutil.virtual_memory().percent / 100\n",
    "                        )\n",
    "\n",
    "                        if current_usage > self.config['memory_critical_threshold']:\n",
    "                            print(\"\\nCRITICAL: Memory usage too high! Forcing cleanup...\")\n",
    "                            self._clear_memory()\n",
    "                        elif current_usage > self.config['memory_warn_threshold']:\n",
    "                            print(\"\\nWARNING: High memory usage detected\")\n",
    "\n",
    "                        time.sleep(5)  # Check every 5 seconds\n",
    "\n",
    "                import threading\n",
    "                monitor_thread = threading.Thread(\n",
    "                    target=memory_monitor,\n",
    "                    daemon=True\n",
    "                )\n",
    "                monitor_thread.start()\n",
    "\n",
    "            setup_memory_monitoring()\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Resource optimization error: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _check_convergence(self):\n",
    "        \"\"\"Check MCMC convergence diagnostics with better error handling\"\"\"\n",
    "        try:\n",
    "            # Get R-hat statistics safely\n",
    "            rhat_stats = pm.rhat(self.trace)\n",
    "            print(\"\\nR-hat statistics:\")\n",
    "            for var in rhat_stats.data_vars:\n",
    "                rhat_value = rhat_stats[var].values\n",
    "                if np.size(rhat_value) == 1:\n",
    "                    print(f\"{var}: {float(rhat_value):.3f}\")\n",
    "                else:\n",
    "                    print(f\"{var}: mean = {np.mean(rhat_value):.3f}, max = {np.max(rhat_value):.3f}\")\n",
    "\n",
    "            # Get effective sample size\n",
    "            ess = pm.ess(self.trace)\n",
    "            print(\"\\nEffective sample sizes:\")\n",
    "            for var in ess.data_vars:\n",
    "                ess_value = ess[var].values\n",
    "                if np.size(ess_value) == 1:\n",
    "                    print(f\"{var}: {float(ess_value):.0f}\")\n",
    "                else:\n",
    "                    print(f\"{var}: mean = {np.mean(ess_value):.0f}, min = {np.min(ess_value):.0f}\")\n",
    "\n",
    "            # Check convergence criteria\n",
    "            max_rhat = max(float(np.max(rhat_stats[var].values)) for var in rhat_stats.data_vars)\n",
    "            min_ess = min(float(np.min(ess[var].values)) for var in ess.data_vars)\n",
    "\n",
    "            print(\"\\nConvergence Summary:\")\n",
    "            print(f\"Maximum R-hat: {max_rhat:.3f} (should be < 1.1)\")\n",
    "            print(f\"Minimum ESS: {min_ess:.0f} (should be > 400)\")\n",
    "\n",
    "            return max_rhat < 1.1 and min_ess > 400\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError checking convergence: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _clear_memory(self):\n",
    "        \"\"\"Clear memory between processing steps\"\"\"\n",
    "        try:\n",
    "            import gc\n",
    "\n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "\n",
    "            # Clear JAX memory if GPU is enabled\n",
    "            if hasattr(self, 'gpu_enabled') and self.gpu_enabled:\n",
    "                try:\n",
    "                    import jax\n",
    "                    jax.clear_caches()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error clearing JAX memory: {str(e)}\")\n",
    "\n",
    "            print(\"\\nMemory cleared\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing memory: {str(e)}\")\n",
    "\n",
    "    # 6. Results and Metrics Methods\n",
    "\n",
    "    def _generate_results(self):\n",
    "        \"\"\"Generate comprehensive analysis results\"\"\"\n",
    "        try:\n",
    "            # Initialize results dictionary\n",
    "            results = {\n",
    "                'parameters': self._extract_parameters(),\n",
    "                'metrics': self._calculate_metrics(),\n",
    "                'diagnostics': self._get_diagnostics(),\n",
    "                'metadata': {\n",
    "                    'n_customers': len(self.data),\n",
    "                    'n_groups': len(self.groups),\n",
    "                    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'model_type': 'Hierarchical BG/NBD',\n",
    "                    'config': self.config\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Add segmentation summary\n",
    "            results['segmentation'] = {\n",
    "                'rfm_segments': self.data['rfm_segment'].value_counts().to_dict(),\n",
    "                'n_cohort_groups': len(self.groups)\n",
    "            }\n",
    "\n",
    "            # Add model performance metrics\n",
    "            if hasattr(self, 'trace'):\n",
    "                div = self.trace.sample_stats.diverging.sum()\n",
    "                results['performance'] = {\n",
    "                    'n_divergent': float(div),\n",
    "                    'percent_divergent': float(div) / len(self.trace.sample_stats.diverging) * 100,\n",
    "                    'n_chains': self.config.get('chains', 2),\n",
    "                    'n_samples': self.config.get('mcmc_samples', 500)\n",
    "                }\n",
    "\n",
    "                # Add log-likelihood if available\n",
    "                if hasattr(self.trace, 'log_likelihood'):\n",
    "                    results['performance']['log_likelihood'] = float(self.trace.log_likelihood.sum())\n",
    "\n",
    "                print(\"\\nResults generated successfully\")\n",
    "                print(f\"Contains {len(results['parameters'])} parameter sets\")\n",
    "                print(f\"Tracked {len(results['metrics'])} metrics\")\n",
    "                print(f\"Generated {len(results['diagnostics'])} diagnostic measures\")\n",
    "\n",
    "                # Print key metrics with proper type handling\n",
    "                print(\"\\nKey Model Metrics:\")\n",
    "                if 'metrics' in results:\n",
    "                    for metric, value in results['metrics'].items():\n",
    "                        if isinstance(value, dict):\n",
    "                            print(f\"\\n{metric}:\")\n",
    "                            for k, v in value.items():\n",
    "                                if isinstance(v, (int, float)):\n",
    "                                    print(f\"  {k}: {v:.4f}\")\n",
    "                                else:\n",
    "                                    print(f\"  {k}: {v}\")\n",
    "                        else:\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                print(f\"{metric}: {value:.4f}\")\n",
    "                            else:\n",
    "                                print(f\"{metric}: {value}\")\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating results: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_parameters(self):\n",
    "        \"\"\"Extract model parameters from trace\"\"\"\n",
    "        params = {}\n",
    "        try:\n",
    "            # Group-level parameters\n",
    "            for param in ['r', 'alpha', 's', 'beta']:\n",
    "                params[param] = {\n",
    "                    'mean': self.trace.posterior[param].mean(dim=['chain', 'draw']).values,\n",
    "                    'std': self.trace.posterior[param].std(dim=['chain', 'draw']).values,\n",
    "                    'hdi_3%': pm.hdi(self.trace.posterior[param], hdi_prob=0.94).values\n",
    "                }\n",
    "\n",
    "            # Covariate coefficients\n",
    "            for param in ['r', 'alpha']:\n",
    "                for cov in self.coef_priors.get(param, {}):\n",
    "                    coef_name = f\"{param}_{cov}_coef\"\n",
    "                    if coef_name in self.trace.posterior:\n",
    "                        params[coef_name] = {\n",
    "                            'mean': float(self.trace.posterior[coef_name].mean()),\n",
    "                            'std': float(self.trace.posterior[coef_name].std()),\n",
    "                            'hdi_94%': pm.hdi(self.trace.posterior[coef_name], hdi_prob=0.94).values\n",
    "                        }\n",
    "\n",
    "            return params\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting parameters: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calculate_metrics(self):\n",
    "        \"\"\"Calculate model performance metrics with robust log-likelihood handling\"\"\"\n",
    "        metrics = {}\n",
    "        try:\n",
    "            # Ensure we have log-likelihood\n",
    "            if hasattr(self.trace, 'log_likelihood'):\n",
    "                try:\n",
    "                    # Try to calculate WAIC\n",
    "                    waic = pm.waic(self.trace, scale='deviance')\n",
    "                    metrics['waic'] = float(waic.waic)\n",
    "                    metrics['waic_se'] = float(waic.waic_se)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not calculate WAIC: {str(e)}\")\n",
    "                    metrics['waic'] = \"Not available\"\n",
    "\n",
    "                try:\n",
    "                    # Try to calculate LOO\n",
    "                    loo = pm.loo(self.trace, scale='deviance')\n",
    "                    metrics['loo'] = float(loo.loo)\n",
    "                    metrics['loo_se'] = float(loo.loo_se)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not calculate LOO: {str(e)}\")\n",
    "                    metrics['loo'] = \"Not available\"\n",
    "\n",
    "                # Calculate log likelihood\n",
    "                try:\n",
    "                    log_like = self.trace.log_likelihood.values\n",
    "                    metrics['log_likelihood'] = float(np.mean(log_like))\n",
    "                    metrics['log_likelihood_std'] = float(np.std(log_like))\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not process log likelihood: {str(e)}\")\n",
    "                    metrics['log_likelihood'] = \"Not available\"\n",
    "            else:\n",
    "                print(\"Warning: No log likelihood found in trace\")\n",
    "                metrics['log_likelihood'] = \"Not available\"\n",
    "                metrics['waic'] = \"Not available\"\n",
    "                metrics['loo'] = \"Not available\"\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    def _get_diagnostics(self):\n",
    "        \"\"\"Get model diagnostics\"\"\"\n",
    "        diagnostics = {}\n",
    "        try:\n",
    "            # MCMC diagnostics\n",
    "            diagnostics['r_hat'] = pm.rhat(self.trace).to_dict()\n",
    "            diagnostics['ess'] = pm.ess(self.trace).to_dict()\n",
    "            diagnostics['mcse'] = pm.mcse(self.trace).to_dict()\n",
    "\n",
    "            # Parameter diagnostics\n",
    "            for param in ['r', 'alpha', 's', 'beta']:\n",
    "                # Compute diagnostics for the specific parameter\n",
    "                r_hat_result = pm.rhat(self.trace.posterior[param])\n",
    "                ess_result = pm.ess(self.trace.posterior[param])\n",
    "\n",
    "                # Ensure r_hat_result and ess_result are xarray.Dataset\n",
    "                if isinstance(r_hat_result, xr.Dataset) and isinstance(ess_result, xr.Dataset):\n",
    "                    # Access the specific data variable within the Dataset\n",
    "                    r_hat_data = r_hat_result[param].values\n",
    "                    ess_data = ess_result[param].values\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected format for r_hat or ess: {r_hat_result}, {ess_result}\")\n",
    "\n",
    "                # Compute diagnostics\n",
    "                diagnostics[f'{param}_diagnostics'] = {\n",
    "                    'mean_r_hat': float(np.mean(r_hat_data)),  # Compute mean safely\n",
    "                    'min_ess': float(np.min(ess_data))        # Compute min safely\n",
    "                }\n",
    "\n",
    "            return diagnostics\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting diagnostics: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Save method with error handling\n",
    "    def save_trained_model(self, prefix='clv_model'):\n",
    "        \"\"\"Save trained model with better error handling\"\"\"\n",
    "        try:\n",
    "            import pickle\n",
    "            import os\n",
    "            from datetime import datetime\n",
    "\n",
    "            # Check model status before saving\n",
    "            is_ready, message = HierarchicalCLVSystem.check_model_status(self)\n",
    "            if not is_ready:\n",
    "                raise ValueError(f\"Cannot save model: {message}\")\n",
    "\n",
    "            # Create directory\n",
    "            os.makedirs('trained_models', exist_ok=True)\n",
    "\n",
    "            # Generate timestamp\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "            base_path = f'trained_models/{prefix}_{timestamp}'\n",
    "\n",
    "            # Save what we can\n",
    "            model_data = {\n",
    "                'config': self.config,\n",
    "                'segment_bins': self.segment_bins,\n",
    "                'segments': self.segments if hasattr(self, 'segments') else None,\n",
    "                'groups': self.groups if hasattr(self, 'groups') else None,\n",
    "                'coords': self.coords if hasattr(self, 'coords') else None,\n",
    "                'scalers': self.scalers if hasattr(self, 'scalers') else None,\n",
    "                'metadata': {\n",
    "                    'save_time': timestamp,\n",
    "                    'model_type': 'Hierarchical BG/NBD',\n",
    "                    'convergence_warning': not self._check_convergence() if hasattr(self, 'trace') else True\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Save model data\n",
    "            data_path = f'{base_path}_data.pkl'\n",
    "            with open(data_path, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            print(f\"\\nModel data saved to: {data_path}\")\n",
    "\n",
    "            # Try to save trace if it exists\n",
    "            if hasattr(self, 'trace') and self.trace is not None:\n",
    "                try:\n",
    "                    trace_path = f'{base_path}_trace.nc'  # Note the .nc extension\n",
    "                    self.trace.to_netcdf(trace_path)  # Use to_netcdf instead of pm.save_trace\n",
    "                    print(f\"Trace saved to: {trace_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not save trace: {str(e)}\")\n",
    "\n",
    "            return base_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # 7. Resource Management Methods\n",
    "\n",
    "    def _setup_gpu(self):\n",
    "        \"\"\"Configure GPU environment\"\"\"\n",
    "        try:\n",
    "            import jax\n",
    "            import numpyro\n",
    "\n",
    "            # Print JAX config\n",
    "            print(\"\\nJAX Configuration:\")\n",
    "            print(f\"JAX version: {jax.__version__}\")\n",
    "            print(f\"Backend: {jax.config.x64_enabled}\")\n",
    "\n",
    "            # Configure JAX\n",
    "            jax.config.update('jax_platform_name', 'gpu')\n",
    "            jax.config.update('jax_enable_x64', True)  # Enable 64-bit precision if needed\n",
    "\n",
    "            # Set device count for NumPyro\n",
    "            numpyro.set_host_device_count(self.config['chains'])\n",
    "\n",
    "            # Check available devices\n",
    "            devices = jax.devices()\n",
    "            print(f\"\\nGPU Setup:\")\n",
    "            print(f\"Available devices: {len(devices)}\")\n",
    "\n",
    "            # Initialize each device\n",
    "            for device in devices:\n",
    "                try:\n",
    "                    # Test device\n",
    "                    jax.device_get(jax.device_put(1, device=device))\n",
    "                    print(f\"Device {device.id} initialized successfully\")\n",
    "\n",
    "                    # Try to get memory info\n",
    "                    if hasattr(device, 'memory_stats'):\n",
    "                        stats = device.memory_stats()\n",
    "                        if stats:\n",
    "                            memory_mb = stats.get('bytes_limit', 0) / (1024 * 1024)\n",
    "                            print(f\"Device memory: {memory_mb:.0f} MB\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not initialize device {device.id}: {str(e)}\")\n",
    "\n",
    "            self.gpu_enabled = True\n",
    "            print(\"\\nGPU setup completed successfully\")\n",
    "\n",
    "        except ImportError as e:\n",
    "            print(f\"\\nJAX/NumPyro import error: {str(e)}\")\n",
    "            print(\"Falling back to CPU.\")\n",
    "            self.config['USE_GPU'] = False\n",
    "            self.gpu_enabled = False\n",
    "        except Exception as e:\n",
    "            print(f\"\\nGPU setup error: {str(e)}\")\n",
    "            print(\"Falling back to CPU.\")\n",
    "            self.config['USE_GPU'] = False\n",
    "            self.gpu_enabled = False\n",
    "\n",
    "\n",
    "    def setup_monitoring(self):\n",
    "        \"\"\"Setup resource monitoring\"\"\"\n",
    "        if not self.config.get('monitor_resources', True):\n",
    "            return\n",
    "\n",
    "        def monitor_loop():\n",
    "            \"\"\"Monitoring loop to track resource usage\"\"\"\n",
    "            while True:\n",
    "                self._monitor_resources()\n",
    "                time.sleep(self.config.get('monitor_interval', 300))  # Adjust the interval as needed\n",
    "\n",
    "        # Start monitoring thread\n",
    "        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)\n",
    "        monitor_thread.start()\n",
    "        print(\"\\nResource monitoring started\")\n",
    "\n",
    "    def _monitor_resources(self):\n",
    "        \"\"\"Monitor system resources\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process()\n",
    "\n",
    "            # Get memory info\n",
    "            memory_info = process.memory_info()\n",
    "            memory_used_gb = memory_info.rss / (1024 * 1024 * 1024)  # Convert to GB\n",
    "            memory_percent = process.memory_percent()\n",
    "\n",
    "            # Get CPU info\n",
    "            cpu_percent = process.cpu_percent(interval=5)  # Increase the interval to 5 seconds\n",
    "\n",
    "            print(\"\\nResource Usage:\")\n",
    "            print(f\"Memory Used: {memory_used_gb:.2f} GB ({memory_percent:.1f}%)\")\n",
    "            print(f\"CPU Usage: {cpu_percent}%\")\n",
    "            print(f\"Active Threads: {process.num_threads()}\")\n",
    "\n",
    "            # Get system-wide memory info\n",
    "            system = psutil.virtual_memory()\n",
    "            print(f\"System Memory: {system.percent}% used\")\n",
    "            print(f\"Available Memory: {system.available / (1024 * 1024 * 1024):.2f} GB\")\n",
    "\n",
    "            # Monitor GPU if enabled\n",
    "            if hasattr(self, 'gpu_enabled') and self.gpu_enabled:\n",
    "                self._monitor_gpu()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Monitoring error: {str(e)}\")\n",
    "\n",
    "    def _monitor_gpu(self):\n",
    "        \"\"\"Monitor GPU memory usage and status\"\"\"\n",
    "        if not hasattr(self, 'gpu_enabled') or not self.gpu_enabled:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import jax\n",
    "            devices = jax.devices()\n",
    "\n",
    "            for device in devices:\n",
    "                try:\n",
    "                    # Get device info\n",
    "                    device_info = jax.device_get(jax.device_put(1, device=device))\n",
    "                    print(f\"\\nGPU {device.id} status: Active\")\n",
    "\n",
    "                    # Try to get memory info if available\n",
    "                    if hasattr(device, 'memory_stats'):\n",
    "                        memory_stats = device.memory_stats()\n",
    "                        if memory_stats:\n",
    "                            used_memory = memory_stats.get('bytes_in_use', 0) / (1024 * 1024)  # Convert to MB\n",
    "                            total_memory = memory_stats.get('bytes_limit', 0) / (1024 * 1024)  # Convert to MB\n",
    "                            print(f\"Memory Usage: {used_memory:.1f}MB / {total_memory:.1f}MB\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not query GPU {device.id}: {str(e)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"GPU monitoring error: {str(e)}\")\n",
    "\n",
    "    def add_sampling_extensions(self):\n",
    "        \"\"\"Add all sampling extensions to the model\"\"\"\n",
    "        # Add parallel tempering\n",
    "        self.add_parallel_tempering()\n",
    "\n",
    "        # Create adaptive Metropolis sampler\n",
    "        self.create_adaptive_metropolis()\n",
    "\n",
    "        # Add diagnostics\n",
    "        self.add_diagnostics()\n",
    "\n",
    "    def run_sampler_comparison(self, n_samples=1000):\n",
    "        \"\"\"Run comparison of different samplers\"\"\"\n",
    "        samplers_to_test = [\n",
    "            SamplerConfig(\"nuts\"),\n",
    "            SamplerConfig(\"metropolis\"),\n",
    "            SamplerConfig(\"slice\"),\n",
    "            SamplerConfig(\"hmc\")\n",
    "        ]\n",
    "\n",
    "        results = self.diagnostics.compare_samplers(\n",
    "            self, samplers_to_test, n_samples\n",
    "        )\n",
    "\n",
    "        print(\"\\nSampler Comparison Results:\")\n",
    "        print(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_diagnostics(self):\n",
    "        \"\"\"Plot diagnostic visualizations\"\"\"\n",
    "        if not hasattr(self, 'diagnostics'):\n",
    "            print(\"No diagnostics available. Run sampler comparison first.\")\n",
    "            return\n",
    "\n",
    "        # Create diagnostic plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot 1: Sampling times\n",
    "        times = [m['sampling_time'] for m in self.diagnostics.performance_metrics.values()]\n",
    "        axes[0,0].bar(self.diagnostics.performance_metrics.keys(), times)\n",
    "        axes[0,0].set_title('Sampling Times')\n",
    "        axes[0,0].set_xticklabels(self.diagnostics.performance_metrics.keys(), rotation=45)\n",
    "\n",
    "        # Plot 2: ESS\n",
    "        ess = [m['mean_ess'] for m in self.diagnostics.performance_metrics.values()]\n",
    "        axes[0,1].bar(self.diagnostics.performance_metrics.keys(), ess)\n",
    "        axes[0,1].set_title('Mean Effective Sample Size')\n",
    "        axes[0,1].set_xticklabels(self.diagnostics.performance_metrics.keys(), rotation=45)\n",
    "\n",
    "        # Plot 3: Efficiency\n",
    "        efficiency = [m['mean_ess']/m['sampling_time'] for m in self.diagnostics.performance_metrics.values()]\n",
    "        axes[1,0].bar(self.diagnostics.performance_metrics.keys(), efficiency)\n",
    "        axes[1,0].set_title('Sampling Efficiency')\n",
    "        axes[1,0].set_xticklabels(self.diagnostics.performance_metrics.keys(), rotation=45)\n",
    "\n",
    "        # Plot 4: R-hat values\n",
    "        r_hats = [m['max_r_hat'] for m in self.diagnostics.performance_metrics.values()]\n",
    "        axes[1,1].bar(self.diagnostics.performance_metrics.keys(), r_hats)\n",
    "        axes[1,1].set_title('Maximum R-hat Values')\n",
    "        axes[1,1].set_xticklabels(self.diagnostics.performance_metrics.keys(), rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "IvqAOsbOnrnp",
   "metadata": {
    "id": "IvqAOsbOnrnp"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "import dataclasses\n",
    "import json\n",
    "import os\n",
    "import psutil\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class CLVConfig:\n",
    "    \"\"\"Unified configuration for CLV analysis\"\"\"\n",
    "    # Experiment tracking\n",
    "    experiment_id: str = None\n",
    "    override_memory_config: bool = False\n",
    "    sample_size: int = 30000\n",
    "    random_seed: int = 42\n",
    "\n",
    "    # Hardware Settings\n",
    "    use_gpu: bool = True\n",
    "    cores: int = 36\n",
    "    max_memory_gb: int = 150\n",
    "    device_batch_size: int = 512\n",
    "\n",
    "    # MCMC Settings\n",
    "    batch_size: int = 10000\n",
    "    mcmc_samples: int = 500\n",
    "    mcmc_tune: int = 200\n",
    "    target_accept: float = 0.995\n",
    "    max_treedepth: int = 15 # TODO: 25, which is quite high. Review trace plots to see if the sampler consistently hits this limit, and increase it slightly if needed. If divergences persist despite higher depth, focus on reparameterization.\n",
    "    chains: int = 2\n",
    "    thinning: int = 0\n",
    "\n",
    "    # Memory Optimization\n",
    "    gradient_accumulation: int = 2\n",
    "    mixed_precision: bool = True\n",
    "    memory_efficient: bool = True\n",
    "    memory_warn_threshold: float = 0.85\n",
    "    memory_critical_threshold: float = 0.95\n",
    "    auto_cleanup_threshold: float = 0.90\n",
    "    cleanup_interval: int = 100\n",
    "\n",
    "    # Training Controls\n",
    "    early_stop_patience: int = 25\n",
    "    early_stop_delta: float = 0.02\n",
    "    min_ess: int = 400\n",
    "    max_rhat: float = 1.05\n",
    "    burn_in: int = 500\n",
    "\n",
    "    # Monitoring\n",
    "    monitor_resources: bool = True\n",
    "    monitor_interval: int = 120\n",
    "\n",
    "    # Segmentation Settings\n",
    "    segment_config: Dict = dataclasses.field(default_factory=lambda: {\n",
    "        'use_rfm': True,\n",
    "        'rfm_bins': {'frequency': 4, 'recency': 4, 'monetary': 4},\n",
    "        'use_engagement': False,\n",
    "        'use_channel': False,\n",
    "        'use_loyalty': False,\n",
    "        'use_cohorts': True,\n",
    "        'cohort_type': 'quarterly',\n",
    "        'merge_small_segments': True,\n",
    "        'min_segment_size': 200,\n",
    "        'use_combined': True\n",
    "    })\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Generate experiment ID if not provided\"\"\"\n",
    "        if not self.experiment_id:\n",
    "            self.experiment_id = f\"clv_experiment_{datetime.now().strftime('%y%m%d%H%M')}\"\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"Manages CLV configurations and experiment tracking\"\"\"\n",
    "    def __init__(self, base_config: Optional[CLVConfig] = None):\n",
    "        self.base_config = base_config or CLVConfig()\n",
    "        self.config_history = []\n",
    "\n",
    "    def setup_config(self, custom_config: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Setup configuration with optional custom overrides\"\"\"\n",
    "        # Start with base config\n",
    "        config = dataclasses.asdict(self.base_config)\n",
    "\n",
    "        # Apply memory optimization if needed\n",
    "        if not (custom_config and custom_config.get('override_memory_config', False)):\n",
    "            memory_config = self._optimize_memory_settings()\n",
    "            config.update(memory_config)\n",
    "\n",
    "        # Apply custom overrides\n",
    "        if custom_config:\n",
    "            config.update(custom_config)\n",
    "\n",
    "        # Save configuration\n",
    "        self._save_config(config)\n",
    "\n",
    "        return config\n",
    "\n",
    "    def _optimize_memory_settings(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get optimized memory settings based on system resources\"\"\"\n",
    "        memory_config = {}\n",
    "        available_memory = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "        # Adjust batch size based on available memory\n",
    "        memory_config['device_batch_size'] = min(\n",
    "            512, int(available_memory * 100)\n",
    "        )\n",
    "\n",
    "        # Adjust chain count based on GPU availability\n",
    "        if torch.cuda.is_available():\n",
    "            memory_config['chains'] = min(\n",
    "                torch.cuda.device_count(),\n",
    "                self.base_config.chains\n",
    "            )\n",
    "\n",
    "        return memory_config\n",
    "\n",
    "    def _save_config(self, config: Dict[str, Any]):\n",
    "        \"\"\"Save configuration to JSON file and history\"\"\"\n",
    "        # Add to history\n",
    "        self.config_history.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': config\n",
    "        })\n",
    "\n",
    "        # Save to file\n",
    "        filename = f\"active_training_config_{config['experiment_id']}.json\"\n",
    "        os.makedirs('configs', exist_ok=True)\n",
    "        with open(os.path.join('configs', filename), 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_config(experiment_id: str) -> CLVConfig:\n",
    "        \"\"\"Load configuration from saved experiment\"\"\"\n",
    "        config_path = os.path.join('configs', f\"active_training_config_{experiment_id}.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        return CLVConfig(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Udrj5iVn5pEz",
   "metadata": {
    "id": "Udrj5iVn5pEz"
   },
   "outputs": [],
   "source": [
    "def run_clv_analysis(\n",
    "    processed_df: pd.DataFrame,\n",
    "    config: Optional[CLVConfig] = None\n",
    ") -> Tuple[Any, dict]:\n",
    "    \"\"\"Run CLV analysis with unified configuration\"\"\"\n",
    "    try:\n",
    "        # Initialize config manager\n",
    "        config_manager = ConfigManager(config or CLVConfig())\n",
    "        active_config = config_manager.setup_config()\n",
    "\n",
    "        print(f\"\\nActive experiment ID: {active_config['experiment_id']}\")\n",
    "\n",
    "        # 1. Initialize system\n",
    "        print(\"\\n=== Initializing CLV System ===\")\n",
    "        clv_system = HierarchicalCLVSystem(\n",
    "            config=active_config,\n",
    "            segment_config=active_config['segment_config']\n",
    "        )\n",
    "\n",
    "        # 2. Data Validation\n",
    "        print(\"\\n=== Validating Input Data ===\")\n",
    "        print(f\"Input shape: {processed_df.shape}\")\n",
    "        print(f\"Memory usage: {processed_df.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "        # Check for missing values\n",
    "        missing = processed_df.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\nMissing values detected:\")\n",
    "            print(missing[missing > 0])\n",
    "\n",
    "        # 3. Create Segments\n",
    "        print(\"\\n=== Creating Segments ===\")\n",
    "        segmented_df = clv_system.create_segments(processed_df)\n",
    "\n",
    "        # 4. Sample data if requested\n",
    "        sample_size = active_config.get('sample_size')\n",
    "        if sample_size:\n",
    "            sampled_df = segmented_df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"\\nSampled {sample_size} records from segmented dataset\")\n",
    "        else:\n",
    "            sampled_df = segmented_df\n",
    "\n",
    "        # 5. Run Analysis\n",
    "        print(\"\\n=== Running Analysis ===\")\n",
    "        model, results = clv_system.run_analysis(processed_df=sampled_df)\n",
    "\n",
    "        if model is None or results is None:\n",
    "            raise ValueError(\"Analysis failed - check error messages above\")\n",
    "\n",
    "        # 5. Model Validation\n",
    "        print(\"\\n=== Validating Model ===\")\n",
    "        # Call the static method correctly\n",
    "        is_ready, message = HierarchicalCLVSystem.check_model_status(model)\n",
    "        if not is_ready:\n",
    "            raise ValueError(f\"Model validation failed: {message}\")\n",
    "\n",
    "        # 6. Save Model\n",
    "        print(\"\\n=== Saving Model ===\")\n",
    "        try:\n",
    "            if is_ready:\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "                saved_path = model.save_trained_model(prefix=f'clv_model_{timestamp}')\n",
    "\n",
    "                # Verify saved files\n",
    "                data_path = f\"{saved_path}_data.pkl\"\n",
    "                trace_path = f\"{saved_path}_trace\"\n",
    "\n",
    "                files_exist = all(os.path.exists(p) for p in [data_path, trace_path])\n",
    "                if files_exist:\n",
    "                    print(\"\\nSave verification:\")\n",
    "                    print(f\"Data file: {os.path.getsize(data_path) / 1024**2:.1f}MB\")\n",
    "                    print(f\"Trace file: {os.path.getsize(trace_path) / 1024**2:.1f}MB\")\n",
    "                else:\n",
    "                    print(\"Warning: Some save files are missing\")\n",
    "            else:\n",
    "                print(f\"Warning: Model not saved - {message}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model saving error: {str(e)}\")\n",
    "            print(\"Continuing without saving...\")\n",
    "\n",
    "        # 7. Results Summary\n",
    "        print(\"\\n=== Analysis Results ===\")\n",
    "        if results:\n",
    "            print(\"\\nModel Overview:\")\n",
    "            print(f\"Customers analyzed: {results['metadata']['n_customers']:,}\")\n",
    "            print(f\"Segments created: {results['metadata']['n_groups']:,}\")\n",
    "\n",
    "            if 'segmentation' in results:\n",
    "                print(\"\\nTop Segments:\")\n",
    "                for segment, count in list(results['segmentation']['rfm_segments'].items())[:5]:\n",
    "                    print(f\"{segment}: {count:,} customers\")\n",
    "\n",
    "            print(\"\\nModel Metrics:\")\n",
    "            if 'metrics' in results:\n",
    "                for metric_name, metric_value in results['metrics'].items():\n",
    "                    # Handle dictionary-type metrics\n",
    "                    if isinstance(metric_value, dict):\n",
    "                        print(f\"\\n{metric_name}:\")\n",
    "                        for k, v in metric_value.items():\n",
    "                            if isinstance(v, (int, float)):\n",
    "                                print(f\"  {k}: {v:.4f}\")\n",
    "                            else:\n",
    "                                print(f\"  {k}: {v}\")\n",
    "                    # Handle scalar metrics\n",
    "                    else:\n",
    "                        if isinstance(metric_value, (int, float)):\n",
    "                            print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "            print(\"\\nConvergence Diagnostics:\")\n",
    "            if 'diagnostics' in results:\n",
    "                diag = results['diagnostics']\n",
    "                # Print R-hat statistics\n",
    "                if 'r_hat' in diag:\n",
    "                    try:\n",
    "                        valid_rhats = [float(rhat) for rhat in diag['r_hat'].values()\n",
    "                                     if isinstance(rhat, (int, float))]\n",
    "                        if valid_rhats:\n",
    "                            max_rhat = max(valid_rhats)\n",
    "                            print(f\"Maximum R-hat: {max_rhat:.3f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not calculate max R-hat: {str(e)}\")\n",
    "\n",
    "                # Print ESS statistics\n",
    "                if 'ess' in diag:\n",
    "                    try:\n",
    "                        valid_ess = [float(ess) for ess in diag['ess'].values()\n",
    "                                   if isinstance(ess, (int, float))]\n",
    "                        if valid_ess:\n",
    "                            min_ess = min(valid_ess)\n",
    "                            print(f\"Minimum ESS: {min_ess:.0f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not calculate min ESS: {str(e)}\")\n",
    "\n",
    "            # Print performance metrics if available\n",
    "            if 'performance' in results:\n",
    "                print(\"\\nPerformance Metrics:\")\n",
    "                perf = results['performance']\n",
    "                for metric_name, metric_value in perf.items():\n",
    "                    if isinstance(metric_value, (int, float)):\n",
    "                        print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "        return model, results\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"\\nValidation Error: {str(ve)}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nExecution Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        print(\"\\n=== Cleanup ===\")\n",
    "        try:\n",
    "            if 'model' in locals() and hasattr(model, '_clear_memory'):\n",
    "                model._clear_memory()\n",
    "            gc.collect()\n",
    "            if hasattr(torch, 'cuda'):\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Memory cleared successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Cleanup warning: {str(e)}\")\n",
    "        print(\"\\n=== Analysis Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dedIVXRiD9ZM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "23e9c67de24d4e8f81fab101910ba783",
      "7b96143314e84bbdad0539c1b7d5fc3e"
     ]
    },
    "id": "dedIVXRiD9ZM",
    "outputId": "9df2d878-add6-4053-9eec-59db16c242de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Active experiment ID: clv_experiment_2411150051\n",
      "\n",
      "=== Initializing CLV System ===\n",
      "\n",
      "JAX Configuration:\n",
      "JAX version: 0.4.35\n",
      "Backend: False\n",
      "\n",
      "GPU Setup:\n",
      "Available devices: 4\n",
      "Device 0 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "Device 1 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "Device 2 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "Device 3 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "\n",
      "GPU setup completed successfully\n",
      "\n",
      "Resource monitoring started\n",
      "\n",
      "=== Validating Input Data ===\n",
      "Input shape: (1200823, 22)\n",
      "Memory usage: 207.3 MB\n",
      "\n",
      "=== Creating Segments ===\n",
      "Creating segments for 1,200,823 customers\n",
      "\n",
      "f_segment distribution:\n",
      "f_segment\n",
      "Frequency 1    1045538\n",
      "Frequency 2      98719\n",
      "Frequency 3      39553\n",
      "Frequency 4      17013\n",
      "Name: count, dtype: int64\n",
      "\n",
      "r_segment distribution:\n",
      "r_segment\n",
      "Recency 1    440116\n",
      "Recency 2    328762\n",
      "Recency 3    254907\n",
      "Recency 4    177038\n",
      "Name: count, dtype: int64\n",
      "\n",
      "m_segment distribution:\n",
      "m_segment\n",
      "Monetary 1    858368\n",
      "Monetary 2    211419\n",
      "Monetary 3     87157\n",
      "Monetary 4     43879\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Created 64 RFM segments\n",
      "\n",
      "Created 24 cohort groups\n",
      "\n",
      "Cohort distribution by quarter:\n",
      "cohort_period      1      2      3      4\n",
      "cohort_year                              \n",
      "2019           45728  66298  58595  63613\n",
      "2020           40524   6474  40635  63945\n",
      "2021           51603  63775  53600  66629\n",
      "2022           53758  60309  60066  59202\n",
      "2023           47920  51412  46093  54704\n",
      "2024           44807  44637  38881  17615\n",
      "\n",
      "Created 24 cohort groups\n",
      "\n",
      "Created 24 total groups with indices\n",
      "\n",
      "Sampled 30000 records from segmented dataset\n",
      "\n",
      "=== Running Analysis ===\n",
      "\n",
      "=== Starting CLV Analysis ===\n",
      "\n",
      "2.1 Processing Data...\n",
      "Initial shape: (30000, 30)\n",
      "Columns: ['cohort_month', 'recency', 'frequency', 'monetary', 'total_revenue', 'revenue_trend', 'avg_transaction_value', 'first_purchase_date', 'last_purchase_date', 'customer_age_days', 'distinct_categories', 'distinct_brands', 'avg_interpurchase_days', 'has_online_purchases', 'has_store_purchases', 'total_discount_amount', 'avg_discount_amount', 'discount_rate', 'sms_active', 'email_active', 'is_loyalty_member', 'loyalty_points', 'f_segment', 'r_segment', 'm_segment', 'rfm_segment', 'cohort_year', 'cohort_period', 'cohort_segment', 'group_idx']\n",
      "\n",
      "2.2 Creating Segments...\n",
      "Creating segments for 30,000 customers\n",
      "\n",
      "f_segment distribution:\n",
      "f_segment\n",
      "Frequency 1    26103\n",
      "Frequency 2     2521\n",
      "Frequency 3      941\n",
      "Frequency 4      435\n",
      "Name: count, dtype: int64\n",
      "\n",
      "r_segment distribution:\n",
      "r_segment\n",
      "Recency 1    11088\n",
      "Recency 2     8189\n",
      "Recency 3     6403\n",
      "Recency 4     4320\n",
      "Name: count, dtype: int64\n",
      "\n",
      "m_segment distribution:\n",
      "m_segment\n",
      "Monetary 1    21363\n",
      "Monetary 2     5337\n",
      "Monetary 3     2201\n",
      "Monetary 4     1099\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Created 63 RFM segments\n",
      "\n",
      "Created 23 cohort groups\n",
      "\n",
      "Cohort distribution by quarter:\n",
      "cohort_period     1     2     3     4\n",
      "cohort_year                          \n",
      "2019           1144  1623  1489  1562\n",
      "2020           1007   164  1006  1613\n",
      "2021           1319  1587  1355  1629\n",
      "2022           1363  1447  1530  1466\n",
      "2023           1206  1293  1137  1391\n",
      "2024           1132  1128   972   437\n",
      "\n",
      "Created 23 cohort groups\n",
      "\n",
      "Created 23 total groups with indices\n",
      "\n",
      "Segmentation Summary:\n",
      "\n",
      "rfm_segment distribution:\n",
      "rfm_segment\n",
      "Recency 1_Frequency 1_Monetary 1    5986\n",
      "Recency 2_Frequency 1_Monetary 1    5771\n",
      "Recency 3_Frequency 1_Monetary 1    5279\n",
      "Recency 4_Frequency 1_Monetary 1    3778\n",
      "Recency 1_Frequency 1_Monetary 2    1707\n",
      "Name: count, dtype: int64\n",
      "\n",
      "cohort_segment distribution:\n",
      "cohort_segment\n",
      "2021_4    1629\n",
      "2019_2    1623\n",
      "2020_4    1613\n",
      "2021_2    1587\n",
      "2019_4    1562\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampled 30000 records from segmented data\n",
      "\n",
      "3. Processing Batches...\n",
      "\n",
      "Processing 30,000 records in 3 batches\n",
      "Processing batch 1/3\n",
      "Processing batch 2/3\n",
      "Processing batch 3/3\n",
      "\n",
      "Checking final data types...\n",
      "cohort_month: datetime64[ns]\n",
      "recency: float32\n",
      "frequency: int32\n",
      "monetary: float32\n",
      "total_revenue: float32\n",
      "revenue_trend: float32\n",
      "avg_transaction_value: float64\n",
      "first_purchase_date: datetime64[ns]\n",
      "last_purchase_date: datetime64[ns]\n",
      "customer_age_days: float32\n",
      "distinct_categories: float32\n",
      "distinct_brands: float32\n",
      "avg_interpurchase_days: float32\n",
      "has_online_purchases: int32\n",
      "has_store_purchases: int32\n",
      "total_discount_amount: float32\n",
      "avg_discount_amount: float32\n",
      "discount_rate: float32\n",
      "sms_active: int32\n",
      "email_active: int32\n",
      "is_loyalty_member: int32\n",
      "loyalty_points: int32\n",
      "f_segment: category\n",
      "r_segment: category\n",
      "m_segment: category\n",
      "rfm_segment: object\n",
      "cohort_year: int32\n",
      "cohort_period: int32\n",
      "cohort_segment: object\n",
      "group_idx: int32\n",
      "\n",
      "Processed Data Summary:\n",
      "Final shape: (30000, 30)\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "5. Building Model...\n",
      "\n",
      "Building model...\n",
      "Preparing model data...\n",
      "\n",
      "Prepared model data:\n",
      "frequency: shape=(30000,), dtype=float32\n",
      "recency: shape=(30000,), dtype=float32\n",
      "T: shape=(30000,), dtype=float32\n",
      "monetary: shape=(30000,), dtype=float32\n",
      "avg_transaction: shape=(30000,), dtype=float32\n",
      "group_idx: shape=(30000,), dtype=int32\n",
      "loyalty_points_scaled: shape=(30000,), dtype=float32\n",
      "avg_interpurchase_days_scaled: shape=(30000,), dtype=float32\n",
      "distinct_categories_scaled: shape=(30000,), dtype=float32\n",
      "distinct_brands_scaled: shape=(30000,), dtype=float32\n",
      "has_online_purchases: shape=(30000,), dtype=float32\n",
      "has_store_purchases: shape=(30000,), dtype=float32\n",
      "sms_active: shape=(30000,), dtype=float32\n",
      "email_active: shape=(30000,), dtype=float32\n",
      "is_loyalty_member: shape=(30000,), dtype=float32\n",
      "Building PyMC model...\n",
      "\n",
      "Added hierarchical priors\n",
      "\n",
      "Added covariate effects\n",
      "\n",
      "Resource Usage:\n",
      "Memory Used: 4.88 GB (2.6%)\n",
      "CPU Usage: 100.6%\n",
      "Active Threads: 341\n",
      "System Memory: 4.4% used\n",
      "Available Memory: 180.34 GB\n",
      "\n",
      "GPU 0 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "\n",
      "GPU 1 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "\n",
      "GPU 2 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "\n",
      "GPU 3 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "Model built successfully\n",
      "\n",
      "6. Training Model...\n",
      "\n",
      "Optimizing GPU resources...\n",
      "\n",
      "JAX Configuration:\n",
      "JAX version: 0.4.35\n",
      "Backend: True\n",
      "\n",
      "GPU Setup:\n",
      "Available devices: 4\n",
      "Device 0 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "Device 1 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "Device 2 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "Device 3 initialized successfully\n",
      "Device memory: 16859 MB\n",
      "\n",
      "GPU setup completed successfully\n",
      "\n",
      "Initial GPU Memory State:\n",
      "Allocated: 0.0 MB\n",
      "Reserved:  0.0 MB\n",
      "\n",
      "GPU Memory Summary:\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 3                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\n",
      "Optimized batch size: 10000\n",
      "\n",
      "Initial GPU Memory State:\n",
      "Allocated: 0.0 MB\n",
      "Reserved:  0.0 MB\n",
      "\n",
      "GPU Memory Summary:\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 3                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\n",
      "Optimized batch size: 10000\n",
      "\n",
      "Training model...\n",
      "\n",
      "GPU 0 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "\n",
      "GPU 1 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "\n",
      "GPU 2 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n",
      "\n",
      "GPU 3 status: Active\n",
      "Memory Usage: 0.0MB / 16858.7MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e9c67de24d4e8f81fab101910ba783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pymc.stats.convergence:There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing log-likelihood...\n",
      "Warning: Could not compute log-likelihood: 'frequency'\n",
      "\n",
      "Sampling error: Arguments to add_groups() must be xr.Dataset, xr.Dataarray or dicts                    (argument 'log_likelihood' was type '<class 'pytensor.tensor.variable.TensorVariable'>')\n",
      "\n",
      "Partial trace available - attempting to salvage results\n",
      "\n",
      "R-hat statistics:\n",
      "alpha: mean = 1.121, max = 1.193\n",
      "alpha_avg_interpurchase_days_scaled_coef: 1.010\n",
      "alpha_distinct_brands_scaled_coef: 1.027\n",
      "alpha_distinct_categories_scaled_coef: 1.001\n",
      "alpha_email_active_coef: 0.996\n",
      "alpha_has_online_purchases_coef: 1.026\n",
      "alpha_has_store_purchases_coef: 1.017\n",
      "alpha_is_loyalty_member_coef: 0.999\n",
      "alpha_loyalty_points_scaled_coef: 1.003\n",
      "alpha_mu: 1.114\n",
      "alpha_sigma: 1.526\n",
      "alpha_sms_active_coef: 1.003\n",
      "beta: mean = 1.010, max = 1.032\n",
      "beta_mu: 1.008\n",
      "beta_sigma: 1.199\n",
      "r: mean = 1.517, max = 1.611\n",
      "r_alpha: 1.560\n",
      "r_avg_interpurchase_days_scaled_coef: 1.007\n",
      "r_beta: 1.222\n",
      "r_distinct_brands_scaled_coef: 1.000\n",
      "r_distinct_categories_scaled_coef: 1.037\n",
      "r_email_active_coef: 1.020\n",
      "r_has_online_purchases_coef: 1.004\n",
      "r_has_store_purchases_coef: 1.017\n",
      "r_is_loyalty_member_coef: 1.004\n",
      "r_loyalty_points_scaled_coef: 1.011\n",
      "r_sms_active_coef: 1.001\n",
      "s: mean = 1.063, max = 1.099\n",
      "s_alpha: 1.157\n",
      "s_beta: 1.006\n",
      "\n",
      "Effective sample sizes:\n",
      "alpha: mean = 16, min = 9\n",
      "alpha_avg_interpurchase_days_scaled_coef: 341\n",
      "alpha_distinct_brands_scaled_coef: 268\n",
      "alpha_distinct_categories_scaled_coef: 459\n",
      "alpha_email_active_coef: 501\n",
      "alpha_has_online_purchases_coef: 200\n",
      "alpha_has_store_purchases_coef: 248\n",
      "alpha_is_loyalty_member_coef: 390\n",
      "alpha_loyalty_points_scaled_coef: 374\n",
      "alpha_mu: 18\n",
      "alpha_sigma: 4\n",
      "alpha_sms_active_coef: 203\n",
      "beta: mean = 64, min = 44\n",
      "beta_mu: 39\n",
      "beta_sigma: 8\n",
      "r: mean = 4, min = 4\n",
      "r_alpha: 4\n",
      "r_avg_interpurchase_days_scaled_coef: 243\n",
      "r_beta: 7\n",
      "r_distinct_brands_scaled_coef: 291\n",
      "r_distinct_categories_scaled_coef: 330\n",
      "r_email_active_coef: 195\n",
      "r_has_online_purchases_coef: 332\n",
      "r_has_store_purchases_coef: 340\n",
      "r_is_loyalty_member_coef: 293\n",
      "r_loyalty_points_scaled_coef: 284\n",
      "r_sms_active_coef: 381\n",
      "s: mean = 29, min = 17\n",
      "s_alpha: 12\n",
      "s_beta: 112\n",
      "\n",
      "Convergence Summary:\n",
      "Maximum R-hat: 1.611 (should be < 1.1)\n",
      "Minimum ESS: 4 (should be > 400)\n",
      "\n",
      "Final convergence check:\n",
      "\n",
      "R-hat statistics:\n",
      "alpha: mean = 1.121, max = 1.193\n",
      "alpha_avg_interpurchase_days_scaled_coef: 1.010\n",
      "alpha_distinct_brands_scaled_coef: 1.027\n",
      "alpha_distinct_categories_scaled_coef: 1.001\n",
      "alpha_email_active_coef: 0.996\n",
      "alpha_has_online_purchases_coef: 1.026\n",
      "alpha_has_store_purchases_coef: 1.017\n",
      "alpha_is_loyalty_member_coef: 0.999\n",
      "alpha_loyalty_points_scaled_coef: 1.003\n",
      "alpha_mu: 1.114\n",
      "alpha_sigma: 1.526\n",
      "alpha_sms_active_coef: 1.003\n",
      "beta: mean = 1.010, max = 1.032\n",
      "beta_mu: 1.008\n",
      "beta_sigma: 1.199\n",
      "r: mean = 1.517, max = 1.611\n",
      "r_alpha: 1.560\n",
      "r_avg_interpurchase_days_scaled_coef: 1.007\n",
      "r_beta: 1.222\n",
      "r_distinct_brands_scaled_coef: 1.000\n",
      "r_distinct_categories_scaled_coef: 1.037\n",
      "r_email_active_coef: 1.020\n",
      "r_has_online_purchases_coef: 1.004\n",
      "r_has_store_purchases_coef: 1.017\n",
      "r_is_loyalty_member_coef: 1.004\n",
      "r_loyalty_points_scaled_coef: 1.011\n",
      "r_sms_active_coef: 1.001\n",
      "s: mean = 1.063, max = 1.099\n",
      "s_alpha: 1.157\n",
      "s_beta: 1.006\n",
      "\n",
      "Effective sample sizes:\n",
      "alpha: mean = 16, min = 9\n",
      "alpha_avg_interpurchase_days_scaled_coef: 341\n",
      "alpha_distinct_brands_scaled_coef: 268\n",
      "alpha_distinct_categories_scaled_coef: 459\n",
      "alpha_email_active_coef: 501\n",
      "alpha_has_online_purchases_coef: 200\n",
      "alpha_has_store_purchases_coef: 248\n",
      "alpha_is_loyalty_member_coef: 390\n",
      "alpha_loyalty_points_scaled_coef: 374\n",
      "alpha_mu: 18\n",
      "alpha_sigma: 4\n",
      "alpha_sms_active_coef: 203\n",
      "beta: mean = 64, min = 44\n",
      "beta_mu: 39\n",
      "beta_sigma: 8\n",
      "r: mean = 4, min = 4\n",
      "r_alpha: 4\n",
      "r_avg_interpurchase_days_scaled_coef: 243\n",
      "r_beta: 7\n",
      "r_distinct_brands_scaled_coef: 291\n",
      "r_distinct_categories_scaled_coef: 330\n",
      "r_email_active_coef: 195\n",
      "r_has_online_purchases_coef: 332\n",
      "r_has_store_purchases_coef: 340\n",
      "r_is_loyalty_member_coef: 293\n",
      "r_loyalty_points_scaled_coef: 284\n",
      "r_sms_active_coef: 381\n",
      "s: mean = 29, min = 17\n",
      "s_alpha: 12\n",
      "s_beta: 112\n",
      "\n",
      "Convergence Summary:\n",
      "Maximum R-hat: 1.611 (should be < 1.1)\n",
      "Minimum ESS: 4 (should be > 400)\n",
      "\n",
      "Memory cleared\n",
      "\n",
      "R-hat statistics:\n",
      "alpha: mean = 1.121, max = 1.193\n",
      "alpha_avg_interpurchase_days_scaled_coef: 1.010\n",
      "alpha_distinct_brands_scaled_coef: 1.027\n",
      "alpha_distinct_categories_scaled_coef: 1.001\n",
      "alpha_email_active_coef: 0.996\n",
      "alpha_has_online_purchases_coef: 1.026\n",
      "alpha_has_store_purchases_coef: 1.017\n",
      "alpha_is_loyalty_member_coef: 0.999\n",
      "alpha_loyalty_points_scaled_coef: 1.003\n",
      "alpha_mu: 1.114\n",
      "alpha_sigma: 1.526\n",
      "alpha_sms_active_coef: 1.003\n",
      "beta: mean = 1.010, max = 1.032\n",
      "beta_mu: 1.008\n",
      "beta_sigma: 1.199\n",
      "r: mean = 1.517, max = 1.611\n",
      "r_alpha: 1.560\n",
      "r_avg_interpurchase_days_scaled_coef: 1.007\n",
      "r_beta: 1.222\n",
      "r_distinct_brands_scaled_coef: 1.000\n",
      "r_distinct_categories_scaled_coef: 1.037\n",
      "r_email_active_coef: 1.020\n",
      "r_has_online_purchases_coef: 1.004\n",
      "r_has_store_purchases_coef: 1.017\n",
      "r_is_loyalty_member_coef: 1.004\n",
      "r_loyalty_points_scaled_coef: 1.011\n",
      "r_sms_active_coef: 1.001\n",
      "s: mean = 1.063, max = 1.099\n",
      "s_alpha: 1.157\n",
      "s_beta: 1.006\n",
      "\n",
      "Effective sample sizes:\n",
      "alpha: mean = 16, min = 9\n",
      "alpha_avg_interpurchase_days_scaled_coef: 341\n",
      "alpha_distinct_brands_scaled_coef: 268\n",
      "alpha_distinct_categories_scaled_coef: 459\n",
      "alpha_email_active_coef: 501\n",
      "alpha_has_online_purchases_coef: 200\n",
      "alpha_has_store_purchases_coef: 248\n",
      "alpha_is_loyalty_member_coef: 390\n",
      "alpha_loyalty_points_scaled_coef: 374\n",
      "alpha_mu: 18\n",
      "alpha_sigma: 4\n",
      "alpha_sms_active_coef: 203\n",
      "beta: mean = 64, min = 44\n",
      "beta_mu: 39\n",
      "beta_sigma: 8\n",
      "r: mean = 4, min = 4\n",
      "r_alpha: 4\n",
      "r_avg_interpurchase_days_scaled_coef: 243\n",
      "r_beta: 7\n",
      "r_distinct_brands_scaled_coef: 291\n",
      "r_distinct_categories_scaled_coef: 330\n",
      "r_email_active_coef: 195\n",
      "r_has_online_purchases_coef: 332\n",
      "r_has_store_purchases_coef: 340\n",
      "r_is_loyalty_member_coef: 293\n",
      "r_loyalty_points_scaled_coef: 284\n",
      "r_sms_active_coef: 381\n",
      "s: mean = 29, min = 17\n",
      "s_alpha: 12\n",
      "s_beta: 112\n",
      "\n",
      "Convergence Summary:\n",
      "Maximum R-hat: 1.611 (should be < 1.1)\n",
      "Minimum ESS: 4 (should be > 400)\n",
      "\n",
      "Warning: Model may not have converged properly\n",
      "Consider adjusting parameters or reducing model complexity\n",
      "\n",
      "7. Generating Results...\n",
      "Warning: No log likelihood found in trace\n",
      "\n",
      "Results generated successfully\n",
      "Contains 22 parameter sets\n",
      "Tracked 3 metrics\n",
      "Generated 7 diagnostic measures\n",
      "\n",
      "Key Model Metrics:\n",
      "log_likelihood: Not available\n",
      "waic: Not available\n",
      "loo: Not available\n",
      "\n",
      "Key Model Metrics:\n",
      "\n",
      "log_likelihood: Not available\n",
      "\n",
      "waic: Not available\n",
      "\n",
      "loo: Not available\n",
      "\n",
      "8. Saving Model...\n",
      "\n",
      "R-hat statistics:\n",
      "alpha: mean = 1.121, max = 1.193\n",
      "alpha_avg_interpurchase_days_scaled_coef: 1.010\n",
      "alpha_distinct_brands_scaled_coef: 1.027\n",
      "alpha_distinct_categories_scaled_coef: 1.001\n",
      "alpha_email_active_coef: 0.996\n",
      "alpha_has_online_purchases_coef: 1.026\n",
      "alpha_has_store_purchases_coef: 1.017\n",
      "alpha_is_loyalty_member_coef: 0.999\n",
      "alpha_loyalty_points_scaled_coef: 1.003\n",
      "alpha_mu: 1.114\n",
      "alpha_sigma: 1.526\n",
      "alpha_sms_active_coef: 1.003\n",
      "beta: mean = 1.010, max = 1.032\n",
      "beta_mu: 1.008\n",
      "beta_sigma: 1.199\n",
      "r: mean = 1.517, max = 1.611\n",
      "r_alpha: 1.560\n",
      "r_avg_interpurchase_days_scaled_coef: 1.007\n",
      "r_beta: 1.222\n",
      "r_distinct_brands_scaled_coef: 1.000\n",
      "r_distinct_categories_scaled_coef: 1.037\n",
      "r_email_active_coef: 1.020\n",
      "r_has_online_purchases_coef: 1.004\n",
      "r_has_store_purchases_coef: 1.017\n",
      "r_is_loyalty_member_coef: 1.004\n",
      "r_loyalty_points_scaled_coef: 1.011\n",
      "r_sms_active_coef: 1.001\n",
      "s: mean = 1.063, max = 1.099\n",
      "s_alpha: 1.157\n",
      "s_beta: 1.006\n",
      "\n",
      "Effective sample sizes:\n",
      "alpha: mean = 16, min = 9\n",
      "alpha_avg_interpurchase_days_scaled_coef: 341\n",
      "alpha_distinct_brands_scaled_coef: 268\n",
      "alpha_distinct_categories_scaled_coef: 459\n",
      "alpha_email_active_coef: 501\n",
      "alpha_has_online_purchases_coef: 200\n",
      "alpha_has_store_purchases_coef: 248\n",
      "alpha_is_loyalty_member_coef: 390\n",
      "alpha_loyalty_points_scaled_coef: 374\n",
      "alpha_mu: 18\n",
      "alpha_sigma: 4\n",
      "alpha_sms_active_coef: 203\n",
      "beta: mean = 64, min = 44\n",
      "beta_mu: 39\n",
      "beta_sigma: 8\n",
      "r: mean = 4, min = 4\n",
      "r_alpha: 4\n",
      "r_avg_interpurchase_days_scaled_coef: 243\n",
      "r_beta: 7\n",
      "r_distinct_brands_scaled_coef: 291\n",
      "r_distinct_categories_scaled_coef: 330\n",
      "r_email_active_coef: 195\n",
      "r_has_online_purchases_coef: 332\n",
      "r_has_store_purchases_coef: 340\n",
      "r_is_loyalty_member_coef: 293\n",
      "r_loyalty_points_scaled_coef: 284\n",
      "r_sms_active_coef: 381\n",
      "s: mean = 29, min = 17\n",
      "s_alpha: 12\n",
      "s_beta: 112\n",
      "\n",
      "Convergence Summary:\n",
      "Maximum R-hat: 1.611 (should be < 1.1)\n",
      "Minimum ESS: 4 (should be > 400)\n",
      "\n",
      "Model data saved to: trained_models/clv_model_20241115_0053_data.pkl\n",
      "Trace saved to: trained_models/clv_model_20241115_0053_trace.nc\n",
      "Model saved to: trained_models/clv_model_20241115_0053\n",
      "\n",
      "=== Analysis Complete ===\n",
      "\n",
      "=== Validating Model ===\n",
      "\n",
      "=== Saving Model ===\n",
      "\n",
      "R-hat statistics:\n",
      "alpha: mean = 1.121, max = 1.193\n",
      "alpha_avg_interpurchase_days_scaled_coef: 1.010\n",
      "alpha_distinct_brands_scaled_coef: 1.027\n",
      "alpha_distinct_categories_scaled_coef: 1.001\n",
      "alpha_email_active_coef: 0.996\n",
      "alpha_has_online_purchases_coef: 1.026\n",
      "alpha_has_store_purchases_coef: 1.017\n",
      "alpha_is_loyalty_member_coef: 0.999\n",
      "alpha_loyalty_points_scaled_coef: 1.003\n",
      "alpha_mu: 1.114\n",
      "alpha_sigma: 1.526\n",
      "alpha_sms_active_coef: 1.003\n",
      "beta: mean = 1.010, max = 1.032\n",
      "beta_mu: 1.008\n",
      "beta_sigma: 1.199\n",
      "r: mean = 1.517, max = 1.611\n",
      "r_alpha: 1.560\n",
      "r_avg_interpurchase_days_scaled_coef: 1.007\n",
      "r_beta: 1.222\n",
      "r_distinct_brands_scaled_coef: 1.000\n",
      "r_distinct_categories_scaled_coef: 1.037\n",
      "r_email_active_coef: 1.020\n",
      "r_has_online_purchases_coef: 1.004\n",
      "r_has_store_purchases_coef: 1.017\n",
      "r_is_loyalty_member_coef: 1.004\n",
      "r_loyalty_points_scaled_coef: 1.011\n",
      "r_sms_active_coef: 1.001\n",
      "s: mean = 1.063, max = 1.099\n",
      "s_alpha: 1.157\n",
      "s_beta: 1.006\n",
      "\n",
      "Effective sample sizes:\n",
      "alpha: mean = 16, min = 9\n",
      "alpha_avg_interpurchase_days_scaled_coef: 341\n",
      "alpha_distinct_brands_scaled_coef: 268\n",
      "alpha_distinct_categories_scaled_coef: 459\n",
      "alpha_email_active_coef: 501\n",
      "alpha_has_online_purchases_coef: 200\n",
      "alpha_has_store_purchases_coef: 248\n",
      "alpha_is_loyalty_member_coef: 390\n",
      "alpha_loyalty_points_scaled_coef: 374\n",
      "alpha_mu: 18\n",
      "alpha_sigma: 4\n",
      "alpha_sms_active_coef: 203\n",
      "beta: mean = 64, min = 44\n",
      "beta_mu: 39\n",
      "beta_sigma: 8\n",
      "r: mean = 4, min = 4\n",
      "r_alpha: 4\n",
      "r_avg_interpurchase_days_scaled_coef: 243\n",
      "r_beta: 7\n",
      "r_distinct_brands_scaled_coef: 291\n",
      "r_distinct_categories_scaled_coef: 330\n",
      "r_email_active_coef: 195\n",
      "r_has_online_purchases_coef: 332\n",
      "r_has_store_purchases_coef: 340\n",
      "r_is_loyalty_member_coef: 293\n",
      "r_loyalty_points_scaled_coef: 284\n",
      "r_sms_active_coef: 381\n",
      "s: mean = 29, min = 17\n",
      "s_alpha: 12\n",
      "s_beta: 112\n",
      "\n",
      "Convergence Summary:\n",
      "Maximum R-hat: 1.611 (should be < 1.1)\n",
      "Minimum ESS: 4 (should be > 400)\n",
      "\n",
      "Model data saved to: trained_models/clv_model_20241115_0053_20241115_0053_data.pkl\n",
      "Trace saved to: trained_models/clv_model_20241115_0053_20241115_0053_trace.nc\n",
      "Warning: Some save files are missing\n",
      "\n",
      "=== Analysis Results ===\n",
      "\n",
      "Model Overview:\n",
      "Customers analyzed: 30,000\n",
      "Segments created: 23\n",
      "\n",
      "Top Segments:\n",
      "Recency 1_Frequency 1_Monetary 1: 5,986 customers\n",
      "Recency 2_Frequency 1_Monetary 1: 5,771 customers\n",
      "Recency 3_Frequency 1_Monetary 1: 5,279 customers\n",
      "Recency 4_Frequency 1_Monetary 1: 3,778 customers\n",
      "Recency 1_Frequency 1_Monetary 2: 1,707 customers\n",
      "\n",
      "Model Metrics:\n",
      "log_likelihood: Not available\n",
      "waic: Not available\n",
      "loo: Not available\n",
      "\n",
      "Convergence Diagnostics:\n",
      "\n",
      "Performance Metrics:\n",
      "n_divergent: 2.0000\n",
      "percent_divergent: 100.0000\n",
      "n_chains: 2.0000\n",
      "n_samples: 500.0000\n",
      "\n",
      "=== Cleanup ===\n",
      "\n",
      "Memory cleared\n",
      "Memory cleared successfully\n",
      "\n",
      "=== Analysis Complete ===\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic usage with defaults and sampling\n",
    "model, results = run_clv_analysis(\n",
    "    processed_df=processed_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "NmqkuDoNcCvw",
   "metadata": {
    "id": "NmqkuDoNcCvw"
   },
   "outputs": [],
   "source": [
    "class CLVPredictor:\n",
    "    \"\"\"Prediction interface for Hierarchical CLV System\"\"\"\n",
    "\n",
    "    def __init__(self, model_system, config=None):\n",
    "        self.model = model_system\n",
    "        self.config = config or {}\n",
    "        self.predictions = {}\n",
    "        self.default_horizons = [30, 60, 90, 180, 365]  # Days\n",
    "\n",
    "    def predict_clv(self, time_horizons=None, uncertainty=True):\n",
    "        \"\"\"\n",
    "        Generate CLV predictions for specified time horizons\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        time_horizons : list, optional\n",
    "            List of time horizons in days\n",
    "        uncertainty : bool, default=True\n",
    "            Whether to include prediction intervals\n",
    "        \"\"\"\n",
    "        try:\n",
    "            horizons = time_horizons or self.default_horizons\n",
    "            print(f\"\\nGenerating predictions for horizons: {horizons} days\")\n",
    "\n",
    "            # Generate predictions for each horizon\n",
    "            results = {}\n",
    "            for horizon in horizons:\n",
    "                results[horizon] = self._predict_horizon(horizon, uncertainty)\n",
    "\n",
    "            # Store predictions\n",
    "            self.predictions = results\n",
    "\n",
    "            return self._format_predictions(results)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _predict_horizon(self, horizon, uncertainty=True):\n",
    "        \"\"\"Generate predictions for a specific time horizon\"\"\"\n",
    "        try:\n",
    "            # Get parameter posterior samples\n",
    "            params = self._get_posterior_samples()\n",
    "\n",
    "            # Calculate expected transactions\n",
    "            exp_transactions = self._calc_expected_transactions(params, horizon)\n",
    "\n",
    "            # Calculate expected value\n",
    "            exp_value = self._calc_expected_value(params, exp_transactions)\n",
    "\n",
    "            if uncertainty:\n",
    "                # Calculate prediction intervals\n",
    "                intervals = self._calc_prediction_intervals(exp_value)\n",
    "                return {\n",
    "                    'expected_value': float(np.mean(exp_value)),\n",
    "                    'lower_95': float(intervals['lower_95']),\n",
    "                    'upper_95': float(intervals['upper_95']),\n",
    "                    'std': float(np.std(exp_value))\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'expected_value': float(np.mean(exp_value))\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting horizon {horizon}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _get_posterior_samples(self):\n",
    "        \"\"\"Extract posterior samples from trained model\"\"\"\n",
    "        try:\n",
    "            # Get samples from trace\n",
    "            samples = {\n",
    "                'r': self.model.trace.posterior['r'].values,\n",
    "                'alpha': self.model.trace.posterior['alpha'].values,\n",
    "                's': self.model.trace.posterior['s'].values,\n",
    "                'beta': self.model.trace.posterior['beta'].values\n",
    "            }\n",
    "\n",
    "            # Get covariate coefficients if they exist\n",
    "            for param in ['r', 'alpha']:\n",
    "                for cov in self.model.coef_priors.get(param, {}):\n",
    "                    coef_name = f\"{param}_{cov}_coef\"\n",
    "                    if coef_name in self.model.trace.posterior:\n",
    "                        samples[coef_name] = self.model.trace.posterior[coef_name].values\n",
    "\n",
    "            return samples\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting posterior samples: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calc_expected_transactions(self, params, horizon):\n",
    "        \"\"\"Calculate expected number of transactions\"\"\"\n",
    "        try:\n",
    "            # Get current customer data\n",
    "            freq = self.model.data['frequency'].values\n",
    "            rec = self.model.data['recency'].values\n",
    "            T = self.model.data['customer_age_days'].values\n",
    "            group_idx = self.model.data['group_idx'].values\n",
    "\n",
    "            # Calculate customer-specific parameters\n",
    "            r = params['r'][:, :, group_idx]  # [chain, draw, customer]\n",
    "            alpha = params['alpha'][:, :, group_idx]\n",
    "\n",
    "            # Calculate probability of being alive\n",
    "            p_alive = self._calc_probability_alive(r, alpha, freq, rec, T)\n",
    "\n",
    "            # Calculate expected transactions\n",
    "            exp_transactions = p_alive * r * (horizon / alpha)\n",
    "\n",
    "            return exp_transactions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating expected transactions: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calc_probability_alive(self, r, alpha, freq, rec, T):\n",
    "        \"\"\"Calculate probability customer is still alive\"\"\"\n",
    "        try:\n",
    "            # Calculate components of the probability\n",
    "            a1 = (alpha + T) ** (r + freq)\n",
    "            a2 = (alpha + rec) ** (r + freq)\n",
    "            b1 = (alpha + T) ** r\n",
    "            b2 = (alpha + rec) ** r\n",
    "\n",
    "            # Calculate probability\n",
    "            p_alive = 1 - (\n",
    "                (a1 - a2) / (a1 + b1)\n",
    "            )\n",
    "\n",
    "            return p_alive\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating alive probability: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calc_expected_value(self, params, exp_transactions):\n",
    "        \"\"\"Calculate expected monetary value\"\"\"\n",
    "        try:\n",
    "            # Get monetary value parameters\n",
    "            s = params['s'][:, :, self.model.data['group_idx'].values]\n",
    "            beta = params['beta'][:, :, self.model.data['group_idx'].values]\n",
    "\n",
    "            # Expected transaction value\n",
    "            exp_value_per_transaction = s / beta\n",
    "\n",
    "            # Total expected value\n",
    "            total_value = exp_transactions * exp_value_per_transaction\n",
    "\n",
    "            return total_value\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating expected value: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _calc_prediction_intervals(self, values):\n",
    "        \"\"\"Calculate prediction intervals\"\"\"\n",
    "        try:\n",
    "            # Calculate percentiles across chains and draws\n",
    "            lower_95 = np.percentile(values, 2.5, axis=(0, 1))\n",
    "            upper_95 = np.percentile(values, 97.5, axis=(0, 1))\n",
    "\n",
    "            return {\n",
    "                'lower_95': lower_95,\n",
    "                'upper_95': upper_95\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating prediction intervals: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _format_predictions(self, results):\n",
    "        \"\"\"Format predictions into a clean DataFrame\"\"\"\n",
    "        try:\n",
    "            # Create prediction DataFrame\n",
    "            pred_data = []\n",
    "            for horizon, metrics in results.items():\n",
    "                pred_data.append({\n",
    "                    'horizon_days': horizon,\n",
    "                    'expected_clv': metrics['expected_value'],\n",
    "                    'lower_95': metrics.get('lower_95', None),\n",
    "                    'upper_95': metrics.get('upper_95', None),\n",
    "                    'std': metrics.get('std', None)\n",
    "                })\n",
    "\n",
    "            predictions = pd.DataFrame(pred_data)\n",
    "\n",
    "            # Add metadata\n",
    "            predictions.attrs['prediction_date'] = pd.Timestamp.now()\n",
    "            predictions.attrs['model_type'] = 'Hierarchical BG/NBD'\n",
    "            predictions.attrs['n_customers'] = len(self.model.data)\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting predictions: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Th4L2u4kWDSi",
   "metadata": {
    "id": "Th4L2u4kWDSi"
   },
   "outputs": [],
   "source": [
    "# Generate predictions Usage\n",
    "def generate_clv_predictions(clv_system, horizons=None):\n",
    "    \"\"\"Generate CLV predictions using trained model\"\"\"\n",
    "    try:\n",
    "        # Create predictor\n",
    "        predictor = CLVPredictor(clv_system)\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = predictor.predict_clv(\n",
    "            time_horizons=horizons,\n",
    "            uncertainty=True\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\nPrediction Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        for _, row in predictions.iterrows():\n",
    "            print(f\"\\nHorizon: {row['horizon_days']} days\")\n",
    "            print(f\"Expected CLV: ${row['expected_clv']:,.2f}\")\n",
    "            print(f\"95% CI: (${row['lower_95']:,.2f}, ${row['upper_95']:,.2f})\")\n",
    "\n",
    "        return predictions, predictor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating predictions: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4Qy7xQjxVizj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "4Qy7xQjxVizj",
    "outputId": "92cca2ef-5f30-4271-c6d8-e860fc9de60e"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_clv_predictions() got an unexpected keyword argument 'processed_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-11fd2315af24>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate predictions and DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m predictions_summary, horizon_preds_df, predictor = generate_clv_predictions(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclv_system\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprocessed_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessed_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhorizons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m365\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_clv_predictions() got an unexpected keyword argument 'processed_df'"
     ]
    }
   ],
   "source": [
    "# Generate predictions and DataFrame\n",
    "predictions_summary, horizon_preds_df, predictor = generate_clv_predictions(\n",
    "    clv_system=model,\n",
    "    processed_df=processed_df,\n",
    "    horizons=[30, 90, 180, 365]\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M')\n",
    "horizon_preds_df.to_csv(f'clv_predictions_{timestamp}.csv', index=False)\n",
    "\n",
    "# Analyze predictions\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "for horizon in [30, 90, 180, 365]:\n",
    "    cols = [f'clv_{horizon}d_expected']\n",
    "    stats = horizon_preds_df[cols].describe()\n",
    "    print(f\"\\n{horizon}-day CLV:\")\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "M7lfUGqFVq9f",
   "metadata": {
    "id": "M7lfUGqFVq9f"
   },
   "outputs": [],
   "source": [
    "@classmethod\n",
    "def load_trained_model(cls, model_path, config=None):\n",
    "    \"\"\"\n",
    "    Load trained model from files with enhanced error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pickle\n",
    "        import arviz as az\n",
    "        import os\n",
    "\n",
    "        # First verify files exist\n",
    "        data_path = f'{model_path}_data.pkl'\n",
    "        trace_path = f'{model_path}_trace.nc'\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "\n",
    "        if not os.path.exists(trace_path):\n",
    "            print(f\"Warning: Trace file not found: {trace_path}\")\n",
    "\n",
    "        # Check file sizes\n",
    "        data_size = os.path.getsize(data_path) / (1024 * 1024)  # MB\n",
    "        print(f\"\\nFile sizes:\")\n",
    "        print(f\"Data file: {data_size:.1f} MB\")\n",
    "\n",
    "        # Try to load data with error handling\n",
    "        try:\n",
    "            with open(data_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "        except (pickle.UnpicklingError, EOFError) as e:\n",
    "            print(f\"Error reading pickle file: {str(e)}\")\n",
    "            print(\"The model file might be corrupted or in wrong format\")\n",
    "            raise\n",
    "\n",
    "        # Create new instance\n",
    "        instance = cls(config=config)\n",
    "\n",
    "        # Update instance attributes with verification\n",
    "        required_attrs = ['config', 'segment_bins', 'segments', 'groups',\n",
    "                        'coords', 'scalers']\n",
    "\n",
    "        for attr in required_attrs:\n",
    "            if attr in model_data:\n",
    "                setattr(instance, attr, model_data[attr])\n",
    "            else:\n",
    "                print(f\"Warning: Missing attribute '{attr}' in saved model\")\n",
    "\n",
    "        # Override with custom config if provided\n",
    "        if config is not None:\n",
    "            instance.config = config\n",
    "\n",
    "        # Load trace if available\n",
    "        if os.path.exists(trace_path):\n",
    "            try:\n",
    "                instance.trace = az.from_netcdf(trace_path)\n",
    "                trace_size = os.path.getsize(trace_path) / (1024 * 1024)\n",
    "                print(f\"Trace file: {trace_size:.1f} MB\")\n",
    "                print(f\"Trace loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load trace: {str(e)}\")\n",
    "\n",
    "        # Print model info\n",
    "        print(f\"\\nLoaded model from: {model_path}\")\n",
    "        if 'metadata' in model_data:\n",
    "            print(f\"Save timestamp: {model_data['metadata']['save_time']}\")\n",
    "            print(f\"Model type: {model_data['metadata'].get('model_type', 'Unknown')}\")\n",
    "\n",
    "        # Verify critical components\n",
    "        if hasattr(instance, 'data') and instance.data is not None:\n",
    "            print(f\"Data shape: {instance.data.shape}\")\n",
    "        if hasattr(instance, 'groups'):\n",
    "            print(f\"Number of groups: {len(instance.groups)}\")\n",
    "\n",
    "        return instance\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile not found: {str(e)}\")\n",
    "        raise\n",
    "    except pickle.UnpicklingError as e:\n",
    "        print(f\"\\nError unpickling model data: {str(e)}\")\n",
    "        print(\"The model file might be corrupted or in wrong format\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3WmUrJNV2ua",
   "metadata": {
    "id": "a3WmUrJNV2ua"
   },
   "outputs": [],
   "source": [
    "# Helper function to find most recent model\n",
    "def get_latest_model_path():\n",
    "    \"\"\"Find the most recently saved model\"\"\"\n",
    "    try:\n",
    "        import glob\n",
    "        import os\n",
    "\n",
    "        # Look for all model data files\n",
    "        model_files = glob.glob('trained_models/clv_model_*_data.pkl')\n",
    "\n",
    "        if not model_files:\n",
    "            return None\n",
    "\n",
    "        # Get most recent file\n",
    "        latest_file = max(model_files, key=os.path.getctime)\n",
    "\n",
    "        # Remove _data.pkl to get base path\n",
    "        base_path = latest_file.replace('_data.pkl', '')\n",
    "\n",
    "        return base_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding latest model: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eOMWEI_0Xwls",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eOMWEI_0Xwls",
    "outputId": "de4c2bb8-f3a1-4328-b970-0640ac9b3d90"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'trained_models/clv_model_20241115_0053_20241115_0053'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_latest_model_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4YP3oT1MV0A3",
   "metadata": {
    "id": "4YP3oT1MV0A3"
   },
   "outputs": [],
   "source": [
    "# Usage with better error handling:\n",
    "try:\n",
    "    # Specify model path\n",
    "    model_path = 'trained_models/clv_model_20241112_0013'\n",
    "\n",
    "    # First verify files\n",
    "    data_path = f'{model_path}_data.pkl'\n",
    "    trace_path = f'{model_path}_trace.nc'\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Model data file not found: {data_path}\")\n",
    "\n",
    "    # Try to load\n",
    "    print(f\"\\nLoading model from: {model_path}\")\n",
    "    loaded_model = HierarchicalCLVSystem.load_trained_model(model_path)\n",
    "\n",
    "    # Verify loaded model\n",
    "    if loaded_model is not None:\n",
    "        print(\"\\nModel loaded successfully\")\n",
    "        print(f\"Config parameters:\")\n",
    "        for key in ['MCMC_SAMPLES', 'CHAINS', 'BATCH_SIZE']:\n",
    "            print(f\"{key}: {loaded_model.config.get(key, 'Not found')}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFile error: {str(e)}\")\n",
    "except pickle.UnpicklingError as e:\n",
    "    print(f\"\\nPickle error: {str(e)}\")\n",
    "    print(\"The model file might be corrupted\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\nDetailed error:\")\n",
    "    print(traceback.format_exc())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CLV_360_241113.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "23e9c67de24d4e8f81fab101910ba783": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_7b96143314e84bbdad0539c1b7d5fc3e",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sampling 2 chains, 2 divergences <span style=\"color: #008000; text-decoration-color: #008000\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span> / <span style=\"color: #808000; text-decoration-color: #808000\">0:01:35</span>\n</pre>\n",
         "text/plain": "Sampling 2 chains, 2 divergences \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m / \u001b[33m0:01:35\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "7b96143314e84bbdad0539c1b7d5fc3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
